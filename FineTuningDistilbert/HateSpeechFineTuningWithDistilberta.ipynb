{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning DistilBERT with your own dataset for multi-classification task\n",
        "\n",
        "As natural language processing (NLP) has rapidly evolved over the past few years, transformer-based architectures have shown tremendous success on various NLP tasks. The attention mechanism that allows the model to capture contextual information has been a significant breakthrough in the field of NLP. One of the popular transformer-based models is BERT (Bidirectional Encoder Representations from Transformers). BERT has been successful in various NLP tasks, such as text classification, named entity recognition, question answering, and many more.\n",
        "\n",
        "In this blog post, we will focus on fine-tuning DistilBERT, a smaller and faster version of BERT, on our own dataset for a multi-classification task. We will cover the following topics in this blog post:\n",
        "\n",
        "* Introduction to tokenizer.\n",
        "* Overview of DistilBERT.\n",
        "* Preparing data for multi-classification task.\n",
        "* Training and evaluating DistilBERT on our own dataset.\n",
        "\n",
        "## Introduction to Tokenizer \n",
        "\n",
        "Tokenization is the process of splitting the text into smaller sub-parts, called tokens, and each token represents a word, a punctuation mark, or a part of a word. In NLP, the tokenizer is one of the essential components, which converts the raw text data into a form that can be processed by the machine learning model. The tokenizer maps each word into a unique integer, which is used as an input to the model.\n",
        "\n",
        "In this blog post, we will use the AutoTokenizer provided by Hugging Face, a popular NLP library. It automatically selects the appropriate tokenizer for the given transformer-based model, and we don't have to worry about selecting the tokenizer manually.\n",
        "\n",
        "## Overview of DistilBERT \n",
        "\n",
        "DistilBERT is a smaller and faster version of BERT that has been pre-trained on large datasets. The main idea behind DistilBERT is to remove some of the redundant parameters of BERT while still maintaining its performance. DistilBERT has fewer parameters and requires less computation power, making it easier to fine-tune on smaller datasets.\n",
        "\n",
        "## Preparing data for multi-classification task \n",
        "\n",
        "To train and evaluate DistilBERT on our own dataset for a multi-classification task, we need to convert our dataset into a format that DistilBERT can understand. In this blog post, we will use a dataset that contains labeled tweets, and we will classify the tweets into three categories: hate speech, offensive language, and neither.\n",
        "\n",
        "First, we will load the dataset using the pandas library and split it into the training, validation, and test sets. We will then create a custom dataset class that will read the data from a CSV file and tokenize each tweet using the AutoTokenizer provided by Hugging Face. Finally, we will create data loaders for the training, validation, and test sets using the PyTorch DataLoader.\n",
        "\n",
        "## Training and evaluating DistilBERT on our own dataset \n",
        "\n",
        "We will use our own dataset: [Hate Speech and Offensive Language Dataset](https://www.kaggle.com/datasets/mrmorj/hate-speech-and-offensive-language-dataset), with thousands of labeled tweets in 3 clases: Hate Speech, Offensive or Neither, class labels are already encoded as 0,1,2 respectively.  And we will load DistilBert using HuggingFace transformers library and then retrain a model based on DistilBert and our own fine tuned data set for a classification task.\n",
        "\n",
        "\n",
        "## Introduction to Transformers\n",
        "\n",
        "Transformers are a type of neural network architecture that has been used to achieve state-of-the-art performance on a wide range of natural language processing tasks, including sentiment analysis. Transformers are designed to process sequences of input data (e.g., sequences of words in a sentence) by leveraging attention mechanisms to selectively focus on different parts of the input sequence. This allows the model to capture long-range dependencies between different parts of the input sequence, which can be important for tasks such as sentiment analysis.\n",
        "\n",
        "## The Dataset\n",
        "\n",
        "For this tutorial, we will be using a dataset of tweets labeled with sentiment scores. The dataset contains thousands of  tweets labeled with one of three sentiment scores: 0 (hate specch), 1 (offensive), or 2 (neither). We will use this dataset to train and evaluate our sentiment analysis model.\n",
        "\n",
        "## Preprocessing the Data\n",
        "\n",
        "We will start by preprocessing the data, which involves loading the dataset and partitioning it into training, validation, and testing sets. We will use the load_dataset_into_dataframe() function to load the dataset from a CSV file and convert it into a pandas DataFrame.\n",
        "\n",
        "```\n",
        "df = load_dataset_into_dataframe()\n",
        "```\n",
        "\n",
        "Next, we will partition the data into training, validation, and testing sets using the partition_dataset() function. This function shuffles the data and partitions it into three sets based on a 70/15/15 split.\n",
        "\n",
        "```\n",
        "partition_dataset(df)\n",
        "```\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "Before we can train our sentiment analysis model, we need to tokenize the text data. Tokenization involves breaking up the text into individual words or subwords, which can then be fed into the model as input. We will use the `AutoTokenizer` class from the transformers library to tokenize the text data.\n",
        "\n",
        "```\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "```\n",
        "\n",
        "We will use the `CustomDataset` class to create a PyTorch Dataset object that can be fed into the model during training. The `CustomDataset` class reads the CSV files containing the text data and labels, tokenizes the text using the `tokenizer`, and returns the tokenized data along with the labels.\n",
        "\n",
        "```\n",
        "train_dataset = CustomDataset(\"train.csv\")\n",
        "val_dataset = CustomDataset(\"val.csv\")\n",
        "test_dataset = CustomDataset(\"test.csv\")\n",
        "```\n",
        "\n",
        "## Training the Model\n",
        "\n",
        "We will use the `AutoModelForSequenceClassification` class from the transformers library to create a pre-trained sentiment analysis model. We will then fine-tune the model on our training data using the train() function.\n",
        "\n",
        "```\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "train(5, model, optimizer, train_loader, val_loader, device=\"cuda\")\n",
        "```\n",
        "\n",
        "The train() function trains the model for a given number of epochs using the training data and evaluates the model on the validation data after each epoch. We use the Adam optimizer with a learning rate of 2e-5 and a batch size of 16. \n",
        "\n",
        "## Adam Optimizer\n",
        "\n",
        "Adam optimizer is a popular optimization algorithm used in machine learning to train neural networks. It is a variant of the stochastic gradient descent (SGD) algorithm that uses adaptive learning rates.\n",
        "\n",
        "In simple terms, Adam optimizer adjusts the learning rate of each parameter based on the gradient of the parameter and the historical gradients of that parameter. This helps to ensure that the learning rate is neither too high nor too low, leading to faster convergence during training.\n",
        "\n",
        "To summarize, Adam optimizer is an algorithm used to optimize the weights and biases in a neural network during training, by adapting the learning rate of each parameter based on the gradients and historical gradients. This helps to improve the speed and efficiency of the learning process.\n",
        "\n",
        "## Stochastic gradient descent (SGD) \n",
        "\n",
        "It is a popular optimization algorithm used in machine learning to train neural networks. It is a variant of the gradient descent algorithm that uses mini-batches of training data to update the model's parameters.\n",
        "\n",
        "In simple terms, SGD updates the parameters of the model by calculating the gradient of the loss function with respect to the parameters for a small batch of randomly selected training data. The parameters are then updated using the gradient and a learning rate, which determines the size of the step taken in the direction of the gradient. This process is repeated for multiple batches of training data until the model has learned the patterns in the data.\n",
        "\n",
        "To summarize, SGD is an optimization algorithm that updates the parameters of a neural network using mini-batches of training data and the gradient of the loss function with respect to the parameters. It is an efficient and effective way to train models on large datasets and is widely used in deep learning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enough talk, show me the code:\n",
        "\n",
        "Code is highly commented, enjoy and happy coding"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchmetrics\n",
        "import urllib\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from watermark import watermark\n",
        "from packaging import version\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from datasets import load_dataset, Features, Value, ClassLabel"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421410186
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### reporthook(count, block_size, total_size)\n",
        "\n",
        "The function takes three arguments: count, block_size, and total_size. These are the number of blocks downloaded so far, the size of each block in bytes, and the total size of the file being downloaded, respectively.\n",
        "\n",
        "The global keyword is used to indicate that the variable start_time is defined in the global scope, which means it can be accessed and modified from within the function.\n",
        "\n",
        "If count is zero, the function sets start_time to the current time and returns without writing any progress report. This is because there is no progress to report for the first block, and we only want to report progress for subsequent blocks.\n",
        "\n",
        "If count is greater than zero, the function calculates the elapsed time since the start of the download by subtracting start_time from the current time. It also calculates the size of the downloaded block in bytes by multiplying count and block_size.\n",
        "\n",
        "The function then calculates the download speed in MB/s by dividing the size of the downloaded block in bytes by the elapsed time in seconds, and converting the result to megabytes. It also calculates the percentage of the download completed by multiplying the size of the downloaded block by 100 and dividing by the total size of the file.\n",
        "\n",
        "Finally, the function writes a progress report to stdout using sys.stdout.write(). The progress report includes the percentage of the download completed, the size of the downloaded block in megabytes, the download speed in megabytes per second, and the elapsed time"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import sys\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "\n",
        "    # If this is the first block being downloaded\n",
        "    if count == 0:\n",
        "        # Record the current time\n",
        "        start_time = time.time()\n",
        "        return\n",
        "\n",
        "    # Calculate the elapsed time since the start of the download\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    # Calculate the size of the downloaded block in bytes\n",
        "    progress_size = int(count * block_size)\n",
        "\n",
        "    # Calculate the download speed in MB/s\n",
        "    speed = progress_size / (1024.0**2 * duration)\n",
        "\n",
        "    # Calculate the percentage of the download completed\n",
        "    percent = count * block_size * 100.0 / total_size\n",
        "\n",
        "    # Write the progress report to stdout and flush the buffer\n",
        "    sys.stdout.write(\n",
        "        f\"\\r{int(percent)}% | {progress_size / (1024.**2):.2f} MB \"\n",
        "        f\"| {speed:.2f} MB/s | {duration:.2f} sec elapsed\"\n",
        "    )\n",
        "    sys.stdout.flush()"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421410405
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download_dataset()\n",
        "\n",
        "The given code defines a function called download_dataset() which downloads and reads a CSV file called \"labeled_data2.csv\". Here's a breakdown of what the code does:\n",
        "\n",
        "* The function initializes a variable called data_file and sets its value to \"labeled_data2.csv\".\n",
        "* The code checks whether the file data_file exists in the current working directory by calling `os.path.isfile(data_file)`.\n",
        "* If the file does not exist, the function prints an error message and returns None.\n",
        "* If the file exists, the function opens it in read mode using the with `open(data_file, 'r') as f:` statement. This ensures that the file is properly closed after the data has been read from it.\n",
        "* The `csv.DictReader(f)` function is used to read the file object f and create a dictionary object for each row. The `DictReader` class takes the first row of the CSV file as a header row and uses the values in that row as the keys for each subsequent row's dictionary.\n",
        "* The function then initializes an empty list called data to store the data.\n",
        "* For each row in the file, the function extracts the values for the \"class\" and \"tweet\" keys using `row['class']` and `row['tweet']` respectively.\n",
        "* The function then creates a tuple with the extracted class and tweet values and appends it to the data list.\n",
        "* After all rows have been processed, the function returns the data list."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "def download_dataset():\n",
        "    # Set the file name to download\n",
        "    data_file = \"labeled_data2.csv\"\n",
        "\n",
        "    # Check if the file exists in the current working directory\n",
        "    if not os.path.isfile(data_file):\n",
        "        # If the file does not exist, print an error message and return None\n",
        "        print(f\"Error: {data_file} not found\")\n",
        "        return None\n",
        "\n",
        "    # Open the file in read mode\n",
        "    with open(data_file, 'r') as f:\n",
        "        # Use csv.DictReader to read the file and create dictionaries for each row\n",
        "        reader = csv.DictReader(f)\n",
        "        # Initialize an empty list to store the data\n",
        "        data=[]\n",
        "        # Loop through each row in the file\n",
        "        for row in reader:\n",
        "            # Extract the class and tweet values from the row\n",
        "            class_ = row['class']\n",
        "            tweet = row['tweet']\n",
        "            # Create a tuple with the extracted values and append it to the data list\n",
        "            data.append((class_, tweet))\n",
        "\n",
        "    # Return the data list\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421410934
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load_dataset_into_dataframe()\n",
        "\n",
        "This code defines a function named `load_dataset_into_dataframe()` that loads a CSV file named labeled_data2.csv into a pandas dataframe, shuffles the rows randomly, drops an unnecessary column named \"Unnamed: 0\", and returns the resulting dataframe."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_into_dataframe(): # define function\n",
        "    data_file = \"labeled_data2.csv\" # assign file name to a variable\n",
        "    \n",
        "    try: # try to read the file\n",
        "        df = pd.read_csv(data_file, on_bad_lines='skip') # read csv file with pandas\n",
        "    except FileNotFoundError: # if file is not found\n",
        "        print(f\"Error: {data_file} not found\") # print error message\n",
        "        \n",
        "    df[\"class\"] = df[\"class\"].astype(int) # convert \"class\" column to integer type\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True) # shuffle the rows randomly\n",
        "    df.drop(\"Unnamed: 0\", inplace=True, axis=1) # remove \"Unnamed: 0\" column\n",
        "    print(\"Class distribution:\") # print message\n",
        "    print(df[\"class\"].value_counts()) # print the count of each class\n",
        "    return df # return the dataframe"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421411179
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `partition_dataset()`\n",
        "In the `partition_dataset()` method the number of rows in the shuffled dataset is used to compute the number of rows for each split based on the desired proportions. The num_train variable is set to 70% of the total number of rows, and the `num_val` variable is set to 15% of the total number of rows. The `iloc` method is used to slice the dataframe into the train, validation, and test sets, based on their respective row indices. Finally, the resulting dataframes are saved to separate CSV files."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def partition_dataset(df): # define function that takes a pandas dataframe as input\n",
        "    df_shuffled = df.sample(frac=1, random_state=1).reset_index(drop=True) # shuffle the rows of the dataframe randomly\n",
        "\n",
        "    num_rows = len(df_shuffled) # calculate the number of rows in the shuffled dataframe\n",
        "    num_train = int(num_rows * 0.7) # calculate the number of rows to include in the training set (70% of total rows)\n",
        "    num_val = int(num_rows * 0.15) # calculate the number of rows to include in the validation set (15% of total rows)\n",
        "\n",
        "    df_train = df_shuffled.iloc[:num_train] # slice the first num_train rows and assign them to the training set\n",
        "    df_val = df_shuffled.iloc[num_train:num_train+num_val] # slice the next num_val rows and assign them to the validation set\n",
        "    df_test = df_shuffled.iloc[num_train+num_val:] # slice the remaining rows and assign them to the test set\n",
        "\n",
        "    df_train.to_csv(\"train.csv\", index=False, encoding=\"utf-8\") # write the training set to a CSV file named \"train.csv\"\n",
        "\n",
        "    df_val.to_csv(\"val.csv\", index=False, encoding=\"utf-8\") # write the validation set to a CSV file named \"val.csv\"\n",
        "##    df_test.to_csv(\"test.csv\", index=False, encoding=\"utf-8\") # write the test set to a CSV file named \"test.csv\""
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421411402
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CustomDataset\n",
        "\n",
        "This code defines a custom dataset class in PyTorch, which inherits from the Dataset class provided by PyTorch. The custom dataset is designed to load data from a CSV file containing labeled text data, and preprocess the text data so that it can be fed into a neural network for training or inference. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path):\n",
        "        \"\"\"\n",
        "        Reads data from a CSV file and stores it in the dataset object.\n",
        "        \"\"\"\n",
        "        self.data = []\n",
        "        self.labels = []  # Store labels separately\n",
        "        with open(file_path, 'r') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                class_ = row['class']  # Extract the class label from the row\n",
        "                tweet = row['tweet']  # Extract the tweet from the row\n",
        "                self.data.append((class_, tweet))  # Store the tweet and class label as a tuple\n",
        "                self.labels.append(class_)  # Store the label in a separate list\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves a single item (a tweet and its corresponding class label) from the dataset at the specified index.\n",
        "        Tokenizes the tweet using the tokenizer object and returns the input IDs, attention mask, and label as a tuple.\n",
        "        \"\"\"\n",
        "        class_, tweet = self.data[index]  # Retrieve the tweet and class label at the specified index\n",
        "        encoding = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=256, padding='max_length', return_tensors='pt', truncation=True)  # Tokenize the tweet using the tokenizer object\n",
        "        input_ids = encoding['input_ids'][0]  # Get the input IDs from the tokenization output\n",
        "        attention_mask = encoding['attention_mask'][0]  # Get the attention mask from the tokenization output\n",
        "        label = int(self.labels[index])  # Retrieve the label for the tweet and convert it to an integer\n",
        "        return input_ids, attention_mask, label  # Return the input IDs, attention mask, and label as a tuple\n"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421411613
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenize_test()\n",
        "\n",
        "The `tokenize_text` function takes in a batch of text data in the form of a dictionary, where the key is \"tweet\" and the value is a string of text to be tokenized. It uses the `tokenizer` object to tokenize the text by converting it into a list of integers, where each integer corresponds to a particular token in the vocabulary of the `tokenizer`.\n",
        "\n",
        "The`truncation=True` parameter tells the `tokenizer` to truncate the sequence of tokens if it exceeds a certain length, while `padding=True` parameter tells the `tokenizer` to pad the sequence of tokens with special tokens (e.g., [PAD]) to ensure that all inputs have the same length.\n",
        "\n",
        "The function then returns the tokenized text as a dictionary with keys \"input_ids\" and \"attention_mask\", which contain the tokenized input sequence and a mask indicating which elements of the sequence should be attended to by the model, respectively.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(batch):\n",
        "    # expects a dictionary `batch` with a key `\"text\"` containing a string of text to be tokenized\n",
        "    return tokenizer(batch[\"tweet\"], truncation=True, padding=True)\n",
        "    # tokenize the text in `batch[\"text\"]` using the `tokenizer` object\n",
        "    # `truncation=True` tells the tokenizer to truncate the sequence of tokens if it exceeds a certain length\n",
        "    # `padding=True` tells the tokenizer to pad the sequence of tokens with special tokens (e.g., `[PAD]`) to ensure that all inputs have the same length\n",
        "    # the function returns the tokenized text as a list of integers, where each integer corresponds to a particular token in the vocabulary of the `tokenizer`"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421411839
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train()\n",
        "\n",
        "The `train()` function trains a neural network model for a given number of epochs on a given dataset. In each epoch, the model is trained on batches of data using an optimizer algorithm such as Adam or SGD. The train_loader and val_loader are data loaders that provide batches of data for training and validation, respectively.\n",
        "\n",
        "During training, the model goes through a forward and backpropagation process to update its weights. The forward propagation computes the model's output for a given input, while the backpropagation computes the gradients of the loss function with respect to the model's parameters. The optimizer uses these gradients to update the model's parameters and minimize the loss function.\n",
        "\n",
        "The `train_acc` and `val_acc `are instances of the Accuracy metric from the torchmetrics library. This metric measures the accuracy of the model's predictions by comparing them to the ground truth labels. In each epoch, the accuracy is calculated for both the training and validation sets.\n",
        "\n",
        "Before training the model, it is initialized with random weights. During training, the model's weights are updated to minimize the loss function. The optimizer.zero_grad() function resets the gradients of all model parameters to zero before the backward propagation step. This is necessary because PyTorch accumulates gradients during backward propagation. By zeroing the gradients, we ensure that we only calculate the gradients for the current batch of data.\n",
        "\n",
        "Finally, the `print()` statements provide information about the training progress. The loss function is printed after each batch of training data, while the accuracy is printed at the end of each epoch. The output of the train() function is the trained model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(num_epochs, model, optimizer, train_loader, val_loader, device):\n",
        "    # trains a neural network model for a given number of epochs on a given dataset\n",
        "    # `num_epochs`: number of epochs to train for\n",
        "    # `model`: neural network model to train\n",
        "    # `optimizer`: optimizer algorithm to use during training (e.g. Adam or SGD)\n",
        "    # `train_loader`: data loader for training set\n",
        "    # `val_loader`: data loader for validation set\n",
        "    # `device`: device to use for computation (e.g. \"cpu\" or \"cuda\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # iterate over each epoch\n",
        "\n",
        "        # initialize a new instance of the Accuracy metric for training set\n",
        "        train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
        "        for batch_idx,batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "            input_ids, attention_mask, label = batch  # unpack the tuple into its elements\n",
        "            # move the tensors to the device\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            label = label.to(device)\n",
        "            # assign the tensors to the batch dictionary\n",
        "            batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n",
        "            \n",
        "            ### FORWARD AND BACK PROP\n",
        "            outputs = model(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"],\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs[\"loss\"].backward()\n",
        "\n",
        "            ### UPDATE MODEL PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            ### LOGGING\n",
        "            #if not batch_idx % 300:\n",
        "            print(f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Loss: {outputs['loss']:.4f}\")\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
        "                train_acc.update(predicted_labels, batch[\"label\"])\n",
        "\n",
        "        ### MORE LOGGING\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
        "            for batch in val_loader:\n",
        "                input_ids, attention_mask, label = batch  # unpack the tuple into its elements\n",
        "                # move the tensors to the device\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                label = label.to(device)\n",
        "                # assign the tensors to the batch dictionary\n",
        "                batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n",
        "           \n",
        "                outputs = model(\n",
        "                    batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"],\n",
        "                    labels=batch[\"label\"],\n",
        "                )\n",
        "                predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
        "                val_acc.update(predicted_labels, batch[\"label\"])\n",
        "\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1:04d}/{num_epochs:04d} | Train acc.: {train_acc.compute()*100:.2f}% | Val acc.: {val_acc.compute()*100:.2f}%\"\n",
        "            )"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421412083
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  fine-tuning a pre-trained model Distilbert\n",
        "\n",
        "The concept of fine-tuning a pre-trained model refers to the process of adjusting the weights of a pre-trained model on a specific task or dataset. The idea behind this is that a pre-trained model has already learned to extract relevant features from a large corpus of text, and we can use that knowledge to improve the performance of the model on a task-specific dataset.\n",
        "\n",
        "In the given code, we begin by loading a dataset, and then proceed to tokenize and numericalize the data using the AutoTokenizer from the transformers library. This involves converting the text data into a numerical representation that can be understood by the model. We then create data loaders using DataLoader from PyTorch, which allow us to feed data to the model in batches during training.\n",
        "\n",
        "Next, we initialize a pre-trained model AutoModelForSequenceClassification from the transformers library, which is a transformer-based model trained on a large corpus of text. We specify the number of labels for our task, and then move the model to the device, which can be a GPU if available for faster computation.\n",
        "\n",
        "Finally, we finetune the model by training it on our task-specific dataset using the train function defined in the code. During training, we update the weights of the model using an optimizer called Adam with a learning rate of 5e-5. We train the model for 3 epochs and then evaluate its performance on the test dataset using the torchmetrics library. The accuracy of the model on the test dataset is printed at the end of the code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(watermark(packages=\"torch,lightning,transformers\", python=True))\n",
        "    print(\"Torch CUDA available?\", torch.cuda.is_available())\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    torch.manual_seed(123)\n",
        "    ##########################\n",
        "    ### 1 Loading the Dataset\n",
        "    ##########################\n",
        "    #download_dataset()\n",
        "    df = load_dataset_into_dataframe()\n",
        "    if not (op.exists(\"train.csv\") and op.exists(\"val.csv\") and op.exists(\"test.csv\")):\n",
        "        partition_dataset(df)\n",
        "\n",
        "    features = Features(\n",
        "        {\n",
        "            \"class\": ClassLabel(\n",
        "                num_classes=3,\n",
        "                names=[\"hate speech\", \"Offensive\", \"Neither\"],\n",
        "            ),\n",
        "            \"tweet\": Value(dtype=\"string\"),\n",
        "        })\n",
        "\n",
        "    hatespeeck_dataset = load_dataset(\n",
        "        \"csv\",\n",
        "        data_files={\n",
        "            \"train\": \"train.csv\",\n",
        "            \"validation\": \"val.csv\",\n",
        "            \"test\": \"test.csv\",\n",
        "        },\n",
        "        features=features\n",
        "    )\n",
        "\n",
        "    #########################################\n",
        "    ### 2 Tokenization and Numericalization\n",
        "    #########################################\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    print(\"Tokenizer input max length:\", tokenizer.model_max_length, flush=True)\n",
        "    print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size, flush=True)\n",
        "    print(\"Tokenizing ...\", flush=True)\n",
        "    hatespeech_tokenized = hatespeeck_dataset.map(tokenize_text, batched=True, batch_size=None)\n",
        "    del hatespeeck_dataset\n",
        "    hatespeech_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"tweet\"])\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "    #########################################\n",
        "    ### 3 Set Up DataLoaders\n",
        "    #########################################\n",
        "    \n",
        "    # Define split ratios and other parameters\n",
        "    train_split = 0.7\n",
        "    validation_split = 0.15\n",
        "    test_split = 0.15\n",
        "    shuffle_dataset = True\n",
        "    random_seed = 42\n",
        "    batch_size = 16\n",
        "\n",
        "    # Create dataset object\n",
        "    dataset = CustomDataset(\"labeled_data2.csv\")\n",
        "\n",
        "    # Implement stratified splits\n",
        "    splitter_train = StratifiedShuffleSplit(n_splits=1, test_size=1 - train_split, random_state=random_seed)\n",
        "    train_indices, remaining_indices = next(splitter_train.split(np.zeros(len(dataset)), dataset.labels))\n",
        "\n",
        "    splitter_test_val = StratifiedShuffleSplit(n_splits=1, test_size=test_split/(test_split+validation_split), random_state=random_seed)\n",
        "    test_indices, val_indices = next(splitter_test_val.split(np.zeros(len(remaining_indices)), [dataset.labels[i] for i in remaining_indices]))\n",
        "\n",
        "    # Create PT data samplers and loaders\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    valid_sampler = SubsetRandomSampler(val_indices)\n",
        "    test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "\n",
        "    #########################################\n",
        "    ### 4 Initializing the Model\n",
        "    #########################################\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\", num_labels=3\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "\n",
        "    #########################################\n",
        "    ### 5 Finetuning\n",
        "    #########################################\n",
        "\n",
        "    start = time.time()\n",
        "    train(\n",
        "        num_epochs=3,\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=validation_loader,\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    end = time.time()\n",
        "    elapsed = end - start\n",
        "    print(f\"Time elapsed {elapsed/60:.2f} min\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3).to(device)\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, label = batch  # unpack the tuple into its elements\n",
        "            # move the tensors to the device\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            label = label.to(device)\n",
        "            # assign the tensors to the batch dictionary\n",
        "            batch = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"label\": label}\n",
        "      \n",
        "            outputs = model(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "                labels=batch[\"label\"],\n",
        "            )\n",
        "            predicted_labels = torch.argmax(outputs[\"logits\"], 1)\n",
        "            test_acc.update(predicted_labels, batch[\"label\"])\n",
        "\n",
        "    print(f\"Test accuracy {test_acc.compute()*100:.2f}%\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Python implementation: CPython\nPython version       : 3.8.5\nIPython version      : 8.4.0\n\ntorch       : 1.12.0\nlightning   : not installed\ntransformers: 4.27.1\n\nTorch CUDA available? True\nClass distribution:\n1    701\n2    157\n0     49\nName: class, dtype: int64\nTokenizer input max length: 512\nTokenizer vocabulary size: 30522\nTokenizing ...\nEpoch: 0001/0003 | Batch 0000/0041 | Loss: 1.0423\nEpoch: 0001/0003 | Batch 0001/0041 | Loss: 0.9934\nEpoch: 0001/0003 | Batch 0002/0041 | Loss: 0.8500\nEpoch: 0001/0003 | Batch 0003/0041 | Loss: 0.8439\nEpoch: 0001/0003 | Batch 0004/0041 | Loss: 0.5773\nEpoch: 0001/0003 | Batch 0005/0041 | Loss: 0.7593\nEpoch: 0001/0003 | Batch 0006/0041 | Loss: 0.5719\nEpoch: 0001/0003 | Batch 0007/0041 | Loss: 0.5381\nEpoch: 0001/0003 | Batch 0008/0041 | Loss: 0.6897\nEpoch: 0001/0003 | Batch 0009/0041 | Loss: 0.9545\nEpoch: 0001/0003 | Batch 0010/0041 | Loss: 0.5337\nEpoch: 0001/0003 | Batch 0011/0041 | Loss: 0.6696\nEpoch: 0001/0003 | Batch 0012/0041 | Loss: 0.5834\nEpoch: 0001/0003 | Batch 0013/0041 | Loss: 0.4556\nEpoch: 0001/0003 | Batch 0014/0041 | Loss: 0.6639\nEpoch: 0001/0003 | Batch 0015/0041 | Loss: 0.8386\nEpoch: 0001/0003 | Batch 0016/0041 | Loss: 0.7255\nEpoch: 0001/0003 | Batch 0017/0041 | Loss: 0.4581\nEpoch: 0001/0003 | Batch 0018/0041 | Loss: 0.6171\nEpoch: 0001/0003 | Batch 0019/0041 | Loss: 0.6489\nEpoch: 0001/0003 | Batch 0020/0041 | Loss: 0.6336\nEpoch: 0001/0003 | Batch 0021/0041 | Loss: 0.4582\nEpoch: 0001/0003 | Batch 0022/0041 | Loss: 0.6124\nEpoch: 0001/0003 | Batch 0023/0041 | Loss: 0.5025\nEpoch: 0001/0003 | Batch 0024/0041 | Loss: 0.5421\nEpoch: 0001/0003 | Batch 0025/0041 | Loss: 0.5283\nEpoch: 0001/0003 | Batch 0026/0041 | Loss: 0.1955\nEpoch: 0001/0003 | Batch 0027/0041 | Loss: 0.9695\nEpoch: 0001/0003 | Batch 0028/0041 | Loss: 0.3371\nEpoch: 0001/0003 | Batch 0029/0041 | Loss: 0.3759\nEpoch: 0001/0003 | Batch 0030/0041 | Loss: 0.3007\nEpoch: 0001/0003 | Batch 0031/0041 | Loss: 0.8365\nEpoch: 0001/0003 | Batch 0032/0041 | Loss: 0.4392\nEpoch: 0001/0003 | Batch 0033/0041 | Loss: 0.6718\nEpoch: 0001/0003 | Batch 0034/0041 | Loss: 0.3197\nEpoch: 0001/0003 | Batch 0035/0041 | Loss: 0.4665\nEpoch: 0001/0003 | Batch 0036/0041 | Loss: 0.1985\nEpoch: 0001/0003 | Batch 0037/0041 | Loss: 0.6053\nEpoch: 0001/0003 | Batch 0038/0041 | Loss: 0.3635\nEpoch: 0001/0003 | Batch 0039/0041 | Loss: 0.3571\nEpoch: 0001/0003 | Batch 0040/0041 | Loss: 0.6474\nEpoch: 0001/0003 | Train acc.: 78.76% | Val acc.: 89.93%\nEpoch: 0002/0003 | Batch 0000/0041 | Loss: 0.1704\nEpoch: 0002/0003 | Batch 0001/0041 | Loss: 0.1313\nEpoch: 0002/0003 | Batch 0002/0041 | Loss: 0.4453\nEpoch: 0002/0003 | Batch 0003/0041 | Loss: 0.2729\nEpoch: 0002/0003 | Batch 0004/0041 | Loss: 0.2386\nEpoch: 0002/0003 | Batch 0005/0041 | Loss: 0.6503\nEpoch: 0002/0003 | Batch 0006/0041 | Loss: 0.4514\nEpoch: 0002/0003 | Batch 0007/0041 | Loss: 0.5827\nEpoch: 0002/0003 | Batch 0008/0041 | Loss: 0.5950\nEpoch: 0002/0003 | Batch 0009/0041 | Loss: 0.3051\nEpoch: 0002/0003 | Batch 0010/0041 | Loss: 0.0571\nEpoch: 0002/0003 | Batch 0011/0041 | Loss: 0.9273\nEpoch: 0002/0003 | Batch 0012/0041 | Loss: 0.6020\nEpoch: 0002/0003 | Batch 0013/0041 | Loss: 0.3947\nEpoch: 0002/0003 | Batch 0014/0041 | Loss: 0.3076\nEpoch: 0002/0003 | Batch 0015/0041 | Loss: 0.2166\nEpoch: 0002/0003 | Batch 0016/0041 | Loss: 0.6529\nEpoch: 0002/0003 | Batch 0017/0041 | Loss: 0.1716\nEpoch: 0002/0003 | Batch 0018/0041 | Loss: 0.4095\nEpoch: 0002/0003 | Batch 0019/0041 | Loss: 0.3383\nEpoch: 0002/0003 | Batch 0020/0041 | Loss: 0.3851\nEpoch: 0002/0003 | Batch 0021/0041 | Loss: 0.3115\nEpoch: 0002/0003 | Batch 0022/0041 | Loss: 0.5514\nEpoch: 0002/0003 | Batch 0023/0041 | Loss: 0.1349\nEpoch: 0002/0003 | Batch 0024/0041 | Loss: 0.2507\nEpoch: 0002/0003 | Batch 0025/0041 | Loss: 0.2665\nEpoch: 0002/0003 | Batch 0026/0041 | Loss: 0.6985\nEpoch: 0002/0003 | Batch 0027/0041 | Loss: 0.2128\nEpoch: 0002/0003 | Batch 0028/0041 | Loss: 0.2648\nEpoch: 0002/0003 | Batch 0029/0041 | Loss: 0.2000\nEpoch: 0002/0003 | Batch 0030/0041 | Loss: 0.1512\nEpoch: 0002/0003 | Batch 0031/0041 | Loss: 0.5927\nEpoch: 0002/0003 | Batch 0032/0041 | Loss: 0.1552\nEpoch: 0002/0003 | Batch 0033/0041 | Loss: 0.2090\nEpoch: 0002/0003 | Batch 0034/0041 | Loss: 0.2772\nEpoch: 0002/0003 | Batch 0035/0041 | Loss: 0.1217\nEpoch: 0002/0003 | Batch 0036/0041 | Loss: 0.4228\nEpoch: 0002/0003 | Batch 0037/0041 | Loss: 0.6429\nEpoch: 0002/0003 | Batch 0038/0041 | Loss: 0.2789\nEpoch: 0002/0003 | Batch 0039/0041 | Loss: 0.2473\nEpoch: 0002/0003 | Batch 0040/0041 | Loss: 0.8651\nEpoch: 0002/0003 | Train acc.: 88.84% | Val acc.: 92.09%\nEpoch: 0003/0003 | Batch 0000/0041 | Loss: 0.2044\nEpoch: 0003/0003 | Batch 0001/0041 | Loss: 0.0999\nEpoch: 0003/0003 | Batch 0002/0041 | Loss: 0.0658\nEpoch: 0003/0003 | Batch 0003/0041 | Loss: 0.2267\nEpoch: 0003/0003 | Batch 0004/0041 | Loss: 0.0672\nEpoch: 0003/0003 | Batch 0005/0041 | Loss: 0.0494\nEpoch: 0003/0003 | Batch 0006/0041 | Loss: 0.4584\nEpoch: 0003/0003 | Batch 0007/0041 | Loss: 0.1384\nEpoch: 0003/0003 | Batch 0008/0041 | Loss: 0.1736\nEpoch: 0003/0003 | Batch 0009/0041 | Loss: 0.4324\nEpoch: 0003/0003 | Batch 0010/0041 | Loss: 0.0414\nEpoch: 0003/0003 | Batch 0011/0041 | Loss: 0.2452\nEpoch: 0003/0003 | Batch 0012/0041 | Loss: 0.2771\nEpoch: 0003/0003 | Batch 0013/0041 | Loss: 0.2143\nEpoch: 0003/0003 | Batch 0014/0041 | Loss: 0.0375\nEpoch: 0003/0003 | Batch 0015/0041 | Loss: 0.1087\nEpoch: 0003/0003 | Batch 0016/0041 | Loss: 0.2194\nEpoch: 0003/0003 | Batch 0017/0041 | Loss: 0.3339\nEpoch: 0003/0003 | Batch 0018/0041 | Loss: 0.2985\nEpoch: 0003/0003 | Batch 0019/0041 | Loss: 0.2074\nEpoch: 0003/0003 | Batch 0020/0041 | Loss: 0.4006\nEpoch: 0003/0003 | Batch 0021/0041 | Loss: 0.1224\nEpoch: 0003/0003 | Batch 0022/0041 | Loss: 0.2449\nEpoch: 0003/0003 | Batch 0023/0041 | Loss: 0.5087\nEpoch: 0003/0003 | Batch 0024/0041 | Loss: 0.2971\nEpoch: 0003/0003 | Batch 0025/0041 | Loss: 0.2606\nEpoch: 0003/0003 | Batch 0026/0041 | Loss: 0.1731\nEpoch: 0003/0003 | Batch 0027/0041 | Loss: 0.1194\nEpoch: 0003/0003 | Batch 0028/0041 | Loss: 0.2983\nEpoch: 0003/0003 | Batch 0029/0041 | Loss: 0.2132\nEpoch: 0003/0003 | Batch 0030/0041 | Loss: 0.0926\nEpoch: 0003/0003 | Batch 0031/0041 | Loss: 0.4203\nEpoch: 0003/0003 | Batch 0032/0041 | Loss: 0.0570\nEpoch: 0003/0003 | Batch 0033/0041 | Loss: 0.5240\nEpoch: 0003/0003 | Batch 0034/0041 | Loss: 0.2758\nEpoch: 0003/0003 | Batch 0035/0041 | Loss: 0.0495\nEpoch: 0003/0003 | Batch 0036/0041 | Loss: 0.3072\nEpoch: 0003/0003 | Batch 0037/0041 | Loss: 0.0553\nEpoch: 0003/0003 | Batch 0038/0041 | Loss: 0.4485\nEpoch: 0003/0003 | Batch 0039/0041 | Loss: 0.2370\nEpoch: 0003/0003 | Batch 0040/0041 | Loss: 0.0559\nEpoch: 0003/0003 | Train acc.: 93.33% | Val acc.: 95.68%\nTime elapsed 1.41 min\nTest accuracy 97.83%\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found cached dataset csv (/home/azureuser/.cache/huggingface/datasets/csv/default-fcccae54916af5c7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n100%|██████████| 3/3 [00:00<00:00, 72.32it/s]\nLoading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/csv/default-fcccae54916af5c7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-963dbc1b3f34fd40.arrow\nLoading cached processed dataset at /home/azureuser/.cache/huggingface/datasets/csv/default-fcccae54916af5c7/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-91c774380f7a4b7f.arrow\nSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679421905995
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then we save the model as PTH file in our local disk."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Create models directory\n",
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_PATH.mkdir(parents=True, exist_ok= True)\n",
        "\n",
        "# 2. Create model save path\n",
        "MODEL_NAME = \"distilbertafinetuned.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "# 3. Save the model state dict\n",
        "print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
        "torch.save(obj=model.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Saving model to: models/distilbertafinetuned.pth\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679422690072
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally we create the code in order to use the model for predictions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"models/distilbertafinetuned.pth\")\n",
        "\n",
        "# Set device to cuda or cpu\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Sample text to classify\n",
        "text = \"ok all is good\"\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move the input tensors to the device\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "# Make a prediction\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(**inputs)\n",
        "    predicted_class = torch.argmax(outputs.logits).item()\n",
        "    \n",
        "# Print the predicted class\n",
        "print(predicted_class)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2\n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1679424387528
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}