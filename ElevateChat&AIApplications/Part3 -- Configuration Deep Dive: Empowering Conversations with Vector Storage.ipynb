{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 -- Configuration Deep Dive: Empowering Conversations with Vector Storage\n",
    "\n",
    "In order to be able to create LLM Applications using Azure Cogntive Search, we need to setup the components for it, this was explained in the first part, we need Indexes, Indexers, Knowledge Store, Data Sources and Skillsets.  In this part we will create tha backend functions to support all of this.\n",
    "\n",
    "If you prefer, check the backend folder for the entire code, which is also very well documented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_indexes\n",
    "\n",
    "This function is a wrapper on top of all what we have to do, it will be used later from the backend which is implemented as an Azure Function.\n",
    "From here we instantiate the DocumentIndexManager, and then we create the document index resources with the function `create_document_index_resources`\n",
    "\n",
    "- Index.\n",
    "- Indexer.\n",
    "- Datasource.\n",
    "- Skillset with custom skill (OpenAI embedding generator).\n",
    "\n",
    "Remember these resources are tied to the source documents, pdf, word, excel, powerpoint, md, or whatever is supported.  Until this point we dont have any vector storage yet.  However when we create the skillset, we define a knowledge store, this means that the output of the custom skill will be saved into the knowledge store, more on this later.\n",
    "\n",
    "\n",
    "Then we instantiate the ChunkIndexManger which will create the chunk index resources using `create_chunk_index_resources`:\n",
    "\n",
    "- Index.\n",
    "- Indexer.\n",
    "- Datasource.\n",
    "\n",
    "In this second set of resources, the indexer is set to a datasource pointing to our Knowledge Store, remember a Knowledge Store is just a storage account, and there we have the projections generated by the previous step, the projections are actually a lot of JSON files with the embeddings generated in our previous step, you will see this later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunkindexmanager import ChunkIndexManager\n",
    "from documentindexmanager import DocumentIndexManager\n",
    "\n",
    "# Function to create search indexes in Azure Search\n",
    "def create_indexes(prefix, customer_storage_connection_string, container_name, config):\n",
    "    \"\"\"\n",
    "    Function to create search indexes in Azure Search.\n",
    "    \"\"\"\n",
    "    index_manager = DocumentIndexManager()\n",
    "    doc_index_resources = index_manager.create_document_index_resources(prefix, customer_storage_connection_string, container_name, config)\n",
    "    time.sleep(5)\n",
    "    chunk_index_manager = ChunkIndexManager()\n",
    "    chunk_index_resources = chunk_index_manager.create_chunk_index_resources(prefix, config)  # doesnt need config\n",
    "    return {\"doc_index_resources\": doc_index_resources, \"chunk_index_resources\": chunk_index_resources}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Indexing Manager\n",
    "\n",
    "The given code defines a Python class called *DocumentIndexManager* that facilitates the creation, management, and deletion of resources for document indexing using Azure Cognitive Search. This class encapsulates functions to set up a document index, create datasources, skillsets, and indexers, as well as to manage these resources. Let's break down the main components of the code:\n",
    "\n",
    "- **_create_document_index:** This function creates a document index within Azure Cognitive Search. It defines the schema of the index, specifying various fields such as document ID, content, filesize, filepath, and more. It also includes searchable and retrievable attributes to enhance search and retrieval efficiency.\n",
    "\n",
    "- **_create_document_datasource:** This function establishes a blob datasource within Azure Search, allowing documents to be ingested from a specified storage container. The function takes inputs like the index prefix, storage connection string, container name, and Azure Search configuration to create the datasource.\n",
    "\n",
    "- **_create_document_skillset:** This function defines a skillset, which is a set of skills applied to the indexed content to extract meaningful information. It might include skills like open ai embedding, OCR, merging, and image analysis. These skills enhance search accuracy by extracting relevant data from the documents. For our project we used only Open AI Embedding, but the code is there for you to try OCR, Merging and Image Analysis skills.  When Creating a skillset, a Knowledge Store has to be defined also, why? Because the output of a custom skill needs a place to be saved.\n",
    "\n",
    "- **_create_document_indexer:** This function creates an indexer that connects the datasource to the document index. The indexer specifies how data should be processed and ingested into the index, including field mappings and indexing parameters. It utilizes the previously defined skillset to enhance the indexed content.\n",
    "\n",
    "- **create_document_index_resources:** This function orchestrates the creation of all necessary resources for document indexing. It invokes the previously defined functions to create the index, datasource, skillset, and indexer. After setting up these resources, it waits for the indexer to complete its processing.\n",
    "\n",
    "- **delete_document_index_resources:** This function cleans up the resources associated with a document index. It deletes the index, indexer, datasource, skillset, and related components. Additionally, it deletes any knowledge store tables and blobs associated with the index.\n",
    "\n",
    "The **DocumentIndexManager** class aims to provide a comprehensive solution for setting up and managing document indexing in Azure Cognitive Search. It encapsulates the various steps involved in creating an effective search solution for documents. By leveraging this class, developers can streamline the process of creating and managing the resources required for efficient document indexing and retrieval using Azure Cognitive Search.\n",
    "\n",
    "Code is well document, use at your own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    FieldMapping,\n",
    "    FieldMappingFunction,\n",
    "    InputFieldMappingEntry,\n",
    "    OutputFieldMappingEntry,\n",
    "    SearchIndexerSkillset,\n",
    "    SearchIndexerKnowledgeStore,\n",
    "    SearchIndexerKnowledgeStoreProjection,\n",
    "    SearchIndexerKnowledgeStoreFileProjectionSelector,\n",
    "    WebApiSkill,\n",
    "    OcrSkill,\n",
    "    ImageAnalysisSkill,\n",
    "    MergeSkill,\n",
    "    CognitiveServicesAccountKey\n",
    ")\n",
    "\n",
    "from utilities import (\n",
    "    get_index_name,\n",
    "    create_index,\n",
    "    get_datasource_name,\n",
    "    create_blob_datasource,\n",
    "    get_indexer_name,\n",
    "    get_indexer_client,\n",
    "    get_knowledge_store_connection_string,\n",
    "    get_chunk_index_blob_container_name,\n",
    "    wait_for_indexer_completion,\n",
    "    get_index_client,\n",
    "    get_skillset_name\n",
    ")\n",
    "\n",
    "\n",
    "class DocumentIndexManager():\n",
    "    def _create_document_index(self, index_prefix, config):\n",
    "        \"\"\"\n",
    "        Creates a document index in Azure Search with the given index_prefix and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the index name.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            Index: The created document index.\n",
    "        \"\"\"\n",
    "        # Get the name for the index\n",
    "        name = get_index_name(index_prefix)\n",
    "        # Define the fields for the index\n",
    "        fields = [\n",
    "            SimpleField(name=\"document_id\", type=SearchFieldDataType.String, filterable=True, sortable=True, key=True),\n",
    "            SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"filesize\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"metadata_storage_name\", type=SearchFieldDataType.String, filterable=True, retrievable=True),\n",
    "            SimpleField(name=\"metadata_storage_path\", type=SearchFieldDataType.String, retrievable=True),\n",
    "            SearchableField(name=\"merged_content\", type=SearchFieldDataType.String, retrievable=True),\n",
    "            SimpleField(name=\"text\", type=\"Collection(Edm.String)\", retrievable=True, searchable=True),\n",
    "            SimpleField(name=\"layoutText\", type=\"Collection(Edm.String)\", retrievable=True, searchable=True)\n",
    "        ]\n",
    "        # Create the index using the custom utility function\n",
    "        return create_index(name, fields, config=config, vector_search=None, semantic_title_field_name=\"filepath\", semantic_content_field_names=[\"content\"])\n",
    "\n",
    "    def _create_document_datasource(self, index_prefix, storage_connection_string, container_name, config):\n",
    "        \"\"\"\n",
    "        Creates a blob datasource in Azure Search with the given index_prefix, storage_connection_string, container_name, and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the datasource name.\n",
    "            storage_connection_string (str): The connection string for the storage account.\n",
    "            container_name (str): The name of the container to index.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            DataSource: The created blob datasource.\n",
    "        \"\"\"\n",
    "        # Get the name for the datasource\n",
    "        name = get_datasource_name(index_prefix)\n",
    "        # Create the datasource using the custom utility function\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name, config)\n",
    "\n",
    "    def _create_document_skillset(self, index_prefix, config, content_field_name=\"content\"):\n",
    "        \"\"\"\n",
    "        Creates a skillset for a document using Azure Search.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix for the index.\n",
    "            config (dict): The configuration dictionary.\n",
    "            content_field_name (str, optional): The name of the content field. Defaults to \"content\".\n",
    "\n",
    "        Returns:\n",
    "            Skillset: The created skillset.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the endpoint for the embedding skill from the configuration dictionary\n",
    "        embedding_skill_endpoint = config['AZURE_SEARCH_EMBEDDING_SKILL_ENDPOINT']\n",
    "\n",
    "        # Get the name of the skillset\n",
    "        name = get_skillset_name(index_prefix)\n",
    "\n",
    "        # Get the name of the chunk index blob container\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "\n",
    "        # Define the content context\n",
    "        content_context = f\"/document/{content_field_name}\"\n",
    "\n",
    "        # Define the embedding skill\n",
    "        embedding_skill = WebApiSkill(\n",
    "            name=\"chunking-embedding-skill\",\n",
    "            uri=embedding_skill_endpoint,\n",
    "            timeout=\"PT3M\",\n",
    "            batch_size=1,\n",
    "            degree_of_parallelism=1,\n",
    "            context=content_context,\n",
    "            inputs=[\n",
    "                InputFieldMappingEntry(name=\"document_id\", source=\"/document/document_id\"),\n",
    "                InputFieldMappingEntry(name=\"text\", source=content_context),\n",
    "                InputFieldMappingEntry(name=\"filepath\", source=\"/document/filepath\"),\n",
    "                InputFieldMappingEntry(name=\"fieldname\", source=f\"='{content_field_name}'\")\n",
    "            ],\n",
    "            outputs=[OutputFieldMappingEntry(name=\"chunks\", target_name=\"chunks\")]\n",
    "        )\n",
    "\n",
    "        # Define the OCR skill\n",
    "        ocr_skill = OcrSkill(\n",
    "            name=\"ocr-skill\",\n",
    "            context=content_context,\n",
    "            inputs=[InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/*\")],\n",
    "            outputs=[\n",
    "                OutputFieldMappingEntry(name=\"text\", target_name=\"text\"),\n",
    "                OutputFieldMappingEntry(name=\"layoutText\", target_name=\"layoutText\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the merge skill\n",
    "        merge_skill = MergeSkill(\n",
    "            name=\"merge-skill\",\n",
    "            context=\"/document\",\n",
    "            inputs=[\n",
    "                InputFieldMappingEntry(name=\"text\", source=\"/document/content\"),\n",
    "                InputFieldMappingEntry(name=\"itemsToInsert\", source=\"/document/normalized_images/*/text\"),  # Example field\n",
    "                InputFieldMappingEntry(name=\"offsets\", source=\"/document/normalized_images/*/contentOffset\")  # Example field\n",
    "            ],\n",
    "            outputs=[\n",
    "                OutputFieldMappingEntry(name=\"mergedText\", target_name=\"merged_text\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the ImageAnalysisSkill\n",
    "        image_analysis_skill = ImageAnalysisSkill(\n",
    "            name=\"image-analysis-skill\",\n",
    "            context=content_context,\n",
    "            inputs=[InputFieldMappingEntry(name=\"image\", source=\"/document/normalized_images/*\")],  # Add inputs parameter\n",
    "            visual_features=[\"tags\", \"description\"],\n",
    "            outputs=[\n",
    "                OutputFieldMappingEntry(name=\"categories\", target_name=\"categories\"),\n",
    "                OutputFieldMappingEntry(name=\"tags\", target_name=\"tags\"),\n",
    "                OutputFieldMappingEntry(name=\"description\", target_name=\"description\"),\n",
    "                OutputFieldMappingEntry(name=\"faces\", target_name=\"faces\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the knowledge store\n",
    "        knowledge_store = SearchIndexerKnowledgeStore(\n",
    "            storage_connection_string=get_knowledge_store_connection_string(config),\n",
    "            projections=[\n",
    "                SearchIndexerKnowledgeStoreProjection(\n",
    "                    objects=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                        storage_container=chunk_index_blob_container_name,\n",
    "                        generated_key_name=\"id\",\n",
    "                        source_context=f\"{content_context}/chunks/*\",\n",
    "                        inputs=[\n",
    "                            InputFieldMappingEntry(name=\"source_document_id\", source=\"/document/document_id\"),\n",
    "                            InputFieldMappingEntry(name=\"source_document_filepath\", source=\"/document/filepath\"),\n",
    "                            InputFieldMappingEntry(name=\"source_field_name\", source=f\"{content_context}/chunks/*/embedding_metadata/fieldname\"),\n",
    "                            InputFieldMappingEntry(name=\"title\", source=f\"{content_context}/chunks/*/title\"),\n",
    "                            InputFieldMappingEntry(name=\"text\", source=f\"{content_context}/chunks/*/content\"),\n",
    "                            InputFieldMappingEntry(name=\"embedding\", source=f\"{content_context}/chunks/*/embedding_metadata/embedding\"),\n",
    "                            InputFieldMappingEntry(name=\"index\", source=f\"{content_context}/chunks/*/embedding_metadata/index\"),\n",
    "                            InputFieldMappingEntry(name=\"offset\", source=f\"{content_context}/chunks/*/embedding_metadata/offset\"),\n",
    "                            InputFieldMappingEntry(name=\"length\", source=f\"{content_context}/chunks/*/embedding_metadata/length\")\n",
    "                        ]\n",
    "                    )]\n",
    "                ),\n",
    "                SearchIndexerKnowledgeStoreProjection(\n",
    "                    files=[SearchIndexerKnowledgeStoreFileProjectionSelector(\n",
    "                        storage_container=f\"{chunk_index_blob_container_name}images\",\n",
    "                        generated_key_name=\"imagepath\",\n",
    "                        source=\"/document/normalized_images/*\",\n",
    "                        inputs=[]\n",
    "                    )]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Define the cognitive services account\n",
    "        cognitiveservicesaccount = CognitiveServicesAccountKey(description=\"Cognitive Services Account\", key=config['AZURE_SEARCH_COGNITIVE_SERVICES_KEY'])\n",
    "\n",
    "        # Define the skillset\n",
    "        skillset = SearchIndexerSkillset(\n",
    "            name=name,\n",
    "            skills=[embedding_skill], #here more skills can be added\n",
    "            description=name,\n",
    "            knowledge_store=knowledge_store,\n",
    "            cognitive_services_account=cognitiveservicesaccount\n",
    "        )\n",
    "\n",
    "        # Create the skillset using the indexer client\n",
    "        client = get_indexer_client(config)\n",
    "        return client.create_skillset(skillset)\n",
    "\n",
    "    def _create_document_indexer(self, index_prefix, data_source_name, index_name, skillset_name, config, content_field_name=\"content\", generate_page_images=True):\n",
    "        \"\"\"\n",
    "        Creates an indexer in Azure Search with the given index_prefix, data_source_name, index_name, skillset_name, config, content_field_name, and generate_page_images.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the indexer name.\n",
    "            data_source_name (str): The name of the data source to use for the indexer.\n",
    "            index_name (str): The name of the index to use for the indexer.\n",
    "            skillset_name (str): The name of the skillset to use for the indexer.\n",
    "            config (dict): The configuration for the Azure Search service.\n",
    "            content_field_name (str): The name of the content field to use for the indexer. Defaults to \"content\".\n",
    "            generate_page_images (bool): Whether to generate normalized images for each page of the document. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            Indexer: The created indexer.\n",
    "        \"\"\"\n",
    "        # Get the name for the indexer\n",
    "        name = get_indexer_name(index_prefix)\n",
    "\n",
    "        # Define the indexer configuration based on the generate_page_images parameter\n",
    "        indexer_config = {\"dataToExtract\": \"contentAndMetadata\", \"imageAction\": \"generateNormalizedImagePerPage\"} if generate_page_images else {\"dataToExtract\": \"contentAndMetadata\"}\n",
    "\n",
    "        # Define the indexing parameters\n",
    "        parameters = IndexingParameters(max_failed_items=-1, configuration=indexer_config)\n",
    "\n",
    "        # Define the field mappings for the indexer\n",
    "        field_mappings = [\n",
    "            FieldMapping(source_field_name=\"metadata_storage_path\", target_field_name=\"document_id\", mapping_function=FieldMappingFunction(name=\"base64Encode\", parameters=None)),\n",
    "            FieldMapping(source_field_name=\"metadata_storage_name\", target_field_name=\"filepath\"),\n",
    "            FieldMapping(source_field_name=\"metadata_storage_size\", target_field_name=\"filesize\")\n",
    "        ]\n",
    "\n",
    "        # Define the output field mappings for the indexer\n",
    "        output_field_mappings = []\n",
    "\n",
    "        # Create the indexer using the custom utility function\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            skillset_name=skillset_name,\n",
    "            field_mappings=field_mappings,\n",
    "            output_field_mappings=output_field_mappings,\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client(config)\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "    def create_document_index_resources(self, index_prefix, customer_storage_connection_string, customer_container_name, config) -> dict:\n",
    "        \"\"\"\n",
    "        Creates the necessary resources for a document index in Azure Search with the given index_prefix, customer_storage_connection_string, customer_container_name, and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the index, data source, indexer, and skillset names.\n",
    "            customer_storage_connection_string (str): The connection string for the customer's storage account.\n",
    "            customer_container_name (str): The name of the container in the customer's storage account.\n",
    "            config (dict): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the names of the created index, data source, indexer, and skillset.\n",
    "        \"\"\"\n",
    "        # Create the index, data source, skillset, and indexer using the custom utility functions\n",
    "        index_name = self._create_document_index(index_prefix, config).name\n",
    "        data_source_name = self._create_document_datasource(index_prefix, customer_storage_connection_string, customer_container_name, config).name\n",
    "        skillset_name = self._create_document_skillset(index_prefix, config).name\n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_document_indexer(index_prefix, data_source_name, index_name, skillset_name, config=config).name\n",
    "        wait_for_indexer_completion(indexer_name, config=config)\n",
    "\n",
    "        # Return a dictionary containing the names of the created index, data source, indexer, and skillset\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"skillset_name\": skillset_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "    def delete_document_index_resources(self, index_prefix, config):\n",
    "        \"\"\"\n",
    "        Deletes the resources for a document index in Azure Search with the given index_prefix and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix used for the index, data source, indexer, and skillset names.\n",
    "            config (dict): The configuration for the Azure Search service.\n",
    "        \"\"\"\n",
    "        # Get the index and indexer clients using the custom utility functions\n",
    "        index_client = get_index_client(config)\n",
    "        indexer_client = get_indexer_client(config)\n",
    "\n",
    "        # Delete the index, indexer, data source, and skillset using the corresponding client methods\n",
    "        index_client.delete_index(index=get_index_name(index_prefix))\n",
    "        indexer_client.delete_indexer(indexer=get_indexer_name(index_prefix))\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=get_datasource_name(index_prefix))\n",
    "        indexer_client.delete_skillset(skillset=get_skillset_name(index_prefix))\n",
    "\n",
    "        # Delete the knowledge store tables and blobs\n",
    "        knowledge_store_connection_string = get_knowledge_store_connection_string()\n",
    "\n",
    "        # Delete the container directly from storage\n",
    "        try:\n",
    "            blob_service = BlobServiceClient.from_connection_string(knowledge_store_connection_string)\n",
    "            blob_service.delete_container(get_chunk_index_blob_container_name(index_prefix))\n",
    "        except ResourceNotFoundError:\n",
    "            # Handle resource not found error\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChunkIndexManager\n",
    "\n",
    "This code defines a Python class called **ChunkIndexManager**, which facilitates the creation, management, and deletion of resources for chunk indexing using Azure Cognitive Search. This class encapsulates functions for setting up a chunk index, creating datasources, and creating indexers for the chunks of data within documents. Let's break down the main components of the code:\n",
    "\n",
    "- **_create_chunk_index:** This function creates a chunk index within Azure Cognitive Search. Similar to the previous example, it defines the schema of the index with various fields, including id, source_document_id, title, text, embedding, and more. Additionally, it configures a vector search using the HNSW algorithm for the embedding field, which is used to perform similarity searches based on document embeddings.\n",
    "\n",
    "- **_create_chunk_datasource:** This function establishes a blob datasource for the chunk index. It takes inputs such as the index prefix, storage connection string, container name, and Azure Search configuration to create the datasource. This datasource allows the chunks of data (e.g., paragraphs, sections) from documents to be ingested.\n",
    "\n",
    "- **_create_chunk_indexer:** This function creates an indexer for the chunk index. It connects the datasource to the index and specifies indexing parameters, including parsing_mode set to \"json\". The indexer processes the chunks of data from the datasource and indexes them in the chunk index.\n",
    "\n",
    "- **create_chunk_index_resources:** This function orchestrates the creation of resources for chunk indexing. It invokes the previously defined functions to create the chunk index, datasource, and indexer. After setting up these resources, it waits for the indexer to complete its processing.\n",
    "\n",
    "- **delete_chunk_index_resources:** This function cleans up the resources associated with chunk indexing. It deletes the chunk index, indexer, and datasource, as well as their related components.\n",
    "\n",
    "The **ChunkIndexManager** class aims to provide a streamlined solution for setting up and managing chunk-based indexing in Azure Cognitive Search. It encapsulates the steps involved in creating an effective search solution for chunks of data within documents. Developers can use this class to simplify the process of creating and managing resources required for efficient chunk-based indexing and retrieval using Azure Cognitive Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SearchIndexer,\n",
    "    IndexingParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmConfiguration\n",
    ")\n",
    "\n",
    "from utilities import (\n",
    "    get_index_name,\n",
    "    create_index,\n",
    "    get_datasource_name,\n",
    "    create_blob_datasource,\n",
    "    get_indexer_name,\n",
    "    get_indexer_client,\n",
    "    get_knowledge_store_connection_string,\n",
    "    get_chunk_index_blob_container_name,\n",
    "    wait_for_indexer_completion,\n",
    "    get_index_client\n",
    ")\n",
    "\n",
    "\n",
    "class ChunkIndexManager():\n",
    "\n",
    "    def _create_chunk_index(self, index_prefix, config):\n",
    "        \"\"\"\n",
    "        Creates a chunk index in Azure Search with the given index_prefix and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the index name.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            SearchIndex: The created index.\n",
    "        \"\"\"\n",
    "        name = get_index_name(f\"{index_prefix}-chunk\")\n",
    "        vector_search = VectorSearch(\n",
    "            algorithm_configurations=[\n",
    "                VectorSearchAlgorithmConfiguration(\n",
    "                    name=\"my-vector-config\",\n",
    "                    kind=\"hnsw\",\n",
    "                    hnsw_parameters={\n",
    "                        \"m\": 4,\n",
    "                        \"efConstruction\": 400,\n",
    "                        \"efSearch\": 1000,\n",
    "                        \"metric\": \"cosine\"\n",
    "                    }\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String,  filterable=True, sortable=True, key=True),\n",
    "            SimpleField(name=\"source_document_id\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_document_filepath\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"source_field_name\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"index\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"offset\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"length\", type=SearchFieldDataType.Int64),\n",
    "            SimpleField(name=\"hash\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"text\", type=SearchFieldDataType.String),\n",
    "            SearchField(name=\"embedding\",\n",
    "                        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True,\n",
    "                        vector_search_dimensions=1536,\n",
    "                        vector_search_configuration=\"my-vector-config\")\n",
    "        ]\n",
    "        index = create_index(name, fields, vector_search=vector_search, semantic_title_field_name=\"title\", semantic_content_field_names=[\"text\"], config=config)\n",
    "        return index\n",
    "\n",
    "    def _create_chunk_datasource(self, index_prefix, storage_connection_string, container_name, config):\n",
    "        \"\"\"\n",
    "        Creates a blob data source for the chunk index with the given index_prefix, storage_connection_string, container_name, and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the data source name.\n",
    "            storage_connection_string (str): The connection string for the Azure Storage account.\n",
    "            container_name (str): The name of the blob container.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            SearchIndexerDataSource: The created data source.\n",
    "        \"\"\"\n",
    "        name = get_datasource_name(f\"{index_prefix}-chunk\")\n",
    "        return create_blob_datasource(name, storage_connection_string, container_name, config=config)\n",
    "\n",
    "    def _create_chunk_indexer(self, index_prefix, data_source_name, index_name, config):\n",
    "        \"\"\"\n",
    "        Creates an indexer for the chunk index with the given index_prefix, data_source_name, index_name, and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the indexer name.\n",
    "            data_source_name (str): The name of the data source.\n",
    "            index_name (str): The name of the index.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            SearchIndexer: The created indexer.\n",
    "        \"\"\"\n",
    "        name = get_indexer_name(f\"{index_prefix}-chunk\")\n",
    "        parameters = IndexingParameters(configuration={\"parsing_mode\": \"json\"})\n",
    "        indexer = SearchIndexer(\n",
    "            name=name,\n",
    "            data_source_name=data_source_name,\n",
    "            target_index_name=index_name,\n",
    "            parameters=parameters\n",
    "        )\n",
    "        indexer_client = get_indexer_client(config)\n",
    "        return indexer_client.create_indexer(indexer)\n",
    "\n",
    "    def create_chunk_index_resources(self, index_prefix, config) -> dict:\n",
    "        \"\"\"\n",
    "        Creates the resources for the chunk index with the given index_prefix and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix to use for the index, data source, and indexer names.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing information about the created resources.\n",
    "        \"\"\"\n",
    "        chunk_index_storage_connection_string = get_knowledge_store_connection_string(config)\n",
    "        chunk_index_blob_container_name = get_chunk_index_blob_container_name(index_prefix)\n",
    "        index_name = self._create_chunk_index(index_prefix, config).name\n",
    "        data_source_name = self._create_chunk_datasource(index_prefix, chunk_index_storage_connection_string, chunk_index_blob_container_name, config=config).name\n",
    "        time.sleep(5)\n",
    "        indexer_name = self._create_chunk_indexer(index_prefix, data_source_name, index_name, config=config).name\n",
    "        wait_for_indexer_completion(indexer_name, config=config)\n",
    "        return {\"index_name\": index_name, \"data_source_name\": data_source_name, \"indexer_name\": indexer_name}\n",
    "\n",
    "    def delete_chunk_index_resources(self, index_prefix, config):\n",
    "        \"\"\"\n",
    "        Deletes the resources for the chunk index with the given index_prefix and config.\n",
    "\n",
    "        Args:\n",
    "            index_prefix (str): The prefix used for the index, data source, and indexer names.\n",
    "            config (SearchServiceClientConfiguration): The configuration for the Azure Search service.\n",
    "        \"\"\"\n",
    "        index_client = get_index_client(config)\n",
    "        indexer_client = get_indexer_client(config)\n",
    "\n",
    "        index_client.delete_index(index=f\"{index_prefix}-chunk-index\")\n",
    "        indexer_client.delete_indexer(indexer=f\"{index_prefix}-chunk-indexer\")\n",
    "        indexer_client.delete_data_source_connection(data_source_connection=f\"{index_prefix}-chunk-datasource\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "This code provides utility functions and methods for interacting with Azure Cognitive Search services and Azure Blob Storage, particularly focused on managing index, datasource, and indexer resources. Let's break down the key components and functionalities:\n",
    "\n",
    "- **Environment Variable Configuration:**\n",
    "The code starts by retrieving essential configuration values from environment variables. These values include AZURE_SEARCH_SERVICE_ENDPOINT, AZURE_SEARCH_API_KEY (admin key for Azure Search), and AZURE_KNOWLEDGE_STORE_STORAGE_CONNECTION_STRING (connection string for an Azure Knowledge Store, which could be a blob storage).\n",
    "\n",
    "- **Client Functions:**\n",
    "The code defines functions get_index_client and get_indexer_client that return instances of SearchIndexClient and SearchIndexerClient respectively. These clients are used to interact with Azure Cognitive Search indexes and indexers.\n",
    "\n",
    "- **Utility Functions:**\n",
    "Several utility functions are provided to generate resource names and other useful operations:\n",
    "\n",
    "- **get_index_name, get_datasource_name, get_skillset_name, get_indexer_name, get_chunk_index_blob_container_name:** These functions generate the names for Azure Search index, datasource, skillset, indexer, and a blob container for chunk indexing based on an index prefix.\n",
    "\n",
    "- **get_knowledge_store_connection_string:** This function retrieves the connection string for an Azure Knowledge Store (such as a blob storage) from the configuration.\n",
    "\n",
    "- **create_index:** This function creates an Azure Search index with specified fields, vector search settings, and semantic configurations. It utilizes SearchIndexClient to create the index.\n",
    "\n",
    "- **create_blob_datasource:** This function creates an Azure Search datasource for Azure Blob Storage using a REST request. It sets up a connection to a specified blob container and includes a soft delete policy. The SearchIndexerClient is used to manage datasources.\n",
    "\n",
    "- **wait_for_indexer_completion:** This function waits for an Azure Search indexer to complete its indexing process. It polls the indexer status and waits until the indexer completes or encounters a transient failure.\n",
    "\n",
    "The provided code functions as a set of tools and utilities to streamline the creation, management, and monitoring of Azure Cognitive Search resources, particularly focusing on chunk indexing using Azure Blob Storage. Developers can use these utilities to interact with Azure Search services effectively and manage various aspects of the search indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SemanticSettings,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField\n",
    ")\n",
    "\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "AZURE_SEARCH_KEY = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING = os.getenv(\"AZURE_KNOWLEDGE_STORE_STORAGE_CONNECTION_STRING\")\n",
    "\n",
    "\n",
    "def get_index_client(config) -> SearchIndexClient:\n",
    "    \"\"\"Returns a SearchIndexClient object for the specified Azure Search service.\"\"\"\n",
    "    return SearchIndexClient(config['AZURE_SEARCH_SERVICE_ENDPOINT'], AzureKeyCredential(config['AZURE_SEARCH_ADMIN_KEY']))\n",
    "\n",
    "\n",
    "def get_indexer_client(config) -> SearchIndexerClient:\n",
    "    \"\"\"Returns a SearchIndexerClient object for the specified Azure Search service.\"\"\"\n",
    "    return SearchIndexerClient(config['AZURE_SEARCH_SERVICE_ENDPOINT'], AzureKeyCredential(config['AZURE_SEARCH_ADMIN_KEY']))\n",
    "\n",
    "\n",
    "def get_index_name(index_prefix):\n",
    "    \"\"\"Returns the name of an Azure Search index given a prefix.\"\"\"\n",
    "    return f\"{index_prefix}-index\"\n",
    "\n",
    "\n",
    "def get_datasource_name(index_prefix):\n",
    "    \"\"\"Returns the name of an Azure Search datasource given a prefix.\"\"\"\n",
    "    return f\"{index_prefix}-datasource\"\n",
    "\n",
    "\n",
    "def get_skillset_name(index_prefix):\n",
    "    \"\"\"Returns the name of an Azure Search skillset given a prefix.\"\"\"\n",
    "    return f\"{index_prefix}-skillset\"\n",
    "\n",
    "\n",
    "def get_indexer_name(index_prefix):\n",
    "    \"\"\"Returns the name of an Azure Search indexer given a prefix.\"\"\"\n",
    "    return f\"{index_prefix}-indexer\"\n",
    "\n",
    "\n",
    "def get_chunk_index_blob_container_name(index_prefix):\n",
    "    \"\"\"Returns the name of an Azure Blob Storage container for chunk indexing given a prefix.\"\"\"\n",
    "    return f\"{index_prefix}ChunkIndex\".replace('-', '').lower()\n",
    "\n",
    "\n",
    "def get_knowledge_store_connection_string(config):\n",
    "    \"\"\"Returns the connection string for an Azure Knowledge Store.\"\"\"\n",
    "    return config['AZURE_SEARCH_KNOWLEDGE_STORE_CONNECTION_STRING']\n",
    "\n",
    "\n",
    "def create_index(index_name, fields, vector_search, semantic_title_field_name, semantic_content_field_names, config):\n",
    "    \"\"\"Creates an Azure Search index with the specified fields and semantic settings.\"\"\"\n",
    "    semantic_settings = SemanticSettings(\n",
    "        configurations=[SemanticConfiguration(\n",
    "            name='default',\n",
    "            prioritized_fields=PrioritizedFields(\n",
    "                title_field=SemanticField(field_name=semantic_title_field_name), prioritized_content_fields=[SemanticField(field_name=field_name) for field_name in semantic_content_field_names]))])\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_settings=semantic_settings)\n",
    "    index_client = get_index_client(config)\n",
    "    return index_client.create_index(index)\n",
    "\n",
    "\n",
    "def create_blob_datasource(datasource_name, storage_connection_string, container_name, config):\n",
    "    \"\"\"Creates an Azure Search datasource for Azure Blob Storage with the specified connection string and container name.\"\"\"\n",
    "    # This example utilizes a REST request as the python SDK doesn't support the blob soft delete policy yet\n",
    "    api_version = '2023-07-01-Preview'\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'api-key': f'{config[\"AZURE_SEARCH_ADMIN_KEY\"]}'\n",
    "    }\n",
    "    data_source = {\n",
    "        \"name\": datasource_name,\n",
    "        \"type\": \"azureblob\",\n",
    "        \"credentials\": {\"connectionString\": storage_connection_string},\n",
    "        \"container\": {\"name\": container_name},\n",
    "        \"dataDeletionDetectionPolicy\": {\"@odata.type\": \"#Microsoft.Azure.Search.NativeBlobSoftDeleteDeletionDetectionPolicy\"}\n",
    "    }\n",
    "\n",
    "    url = '{}/datasources/{}?api-version={}'.format(config['AZURE_SEARCH_SERVICE_ENDPOINT'], datasource_name, api_version)\n",
    "    requests.put(url, json=data_source, headers=headers)\n",
    "\n",
    "    ds_client = get_indexer_client(config)\n",
    "    return ds_client.get_data_source_connection(datasource_name)\n",
    "\n",
    "\n",
    "def wait_for_indexer_completion(indexer_name, config):\n",
    "    \"\"\"Waits for an Azure Search indexer to complete indexing.\"\"\"\n",
    "    indexer_client = get_indexer_client(config)\n",
    "    # poll status and wait until indexer is complete\n",
    "    status = f\"Indexer {indexer_name} not started yet\"\n",
    "    while (indexer_client.get_indexer_status(indexer_name).last_result is None) or ((status := indexer_client.get_indexer_status(indexer_name).last_result.status) != \"success\"):\n",
    "        print(f\"Indexing status:{status}\")\n",
    "\n",
    "        # It's possible that the indexer may reach a state of transient failure, especially when generating embeddings\n",
    "        # via Open AI. For the purposes of the demo, we'll just break out of the loop and continue with the rest of the steps.\n",
    "        if (status == \"transientFailure\"):\n",
    "            print(f\"Indexer {indexer_name} failed before fully indexing documents\")\n",
    "            break\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "The following are the required pip packages to run the solution, please note that azure-search-document its using a beta version\n",
    "\n",
    "```\n",
    "# DO NOT include azure-functions-worker in this file\n",
    "# The Python Worker is managed by Azure Functions platform\n",
    "# Manually managing azure-functions-worker may cause unexpected issues\n",
    "\n",
    "azure-functions\n",
    "langchain\n",
    "openai\n",
    "openai[datalib]\n",
    "azure-storage-blob\n",
    "azure-identity\n",
    "azure-core\n",
    "unstructured \n",
    "tiktoken\n",
    "#pre release https://pypi.org/project/azure-search-documents/#history\n",
    "azure-search-documents==11.4.0b6\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Azure Cognitive Search offers powerful capabilities for search indexing, but navigating its complexities can be daunting. The utility functions and code snippets provided in this project part offer a practical solution to streamline the creation, configuration, and management of search indexes, datasources, and indexers. By abstracting away intricacies and automating common tasks, developers can focus on building effective search solutions that deliver actionable insights from their data.\n",
    "\n",
    "In a world where data-driven decisions are the driving force behind success, simplifying search indexing processes with Azure Cognitive Search utilities becomes a strategic advantage for businesses aiming to unlock the full potential of their data. By incorporating these utilities into your development workflow, you can accelerate the deployment of search solutions and empower your organization to make informed decisions based on accurate and up-to-date information."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
