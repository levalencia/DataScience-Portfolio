{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Byte Pair Encoding (BPE) for Tokenization: A Step-by-Step Guide\n",
    "\n",
    "In the book “Build a Large Language Model (From Scratch)”, Sebastian Raschka introduces various ways to process text data for large language models (LLMs). In Chapter 2, he covers basic tokenization methods, but also mentions more advanced techniques like Byte Pair Encoding (BPE), noting that implementing it is beyond the book’s scope. However, for those curious about diving deeper, I took on the challenge to implement a simple BPE tokenizer myself.\n",
    "\n",
    "In this article, I will walk you through:\n",
    "\n",
    "## Why BPE is crucial for tokenization.\n",
    "\n",
    "Why avoiding UNK tokens is essential for effective language processing.\n",
    "A simple BPE implementation that breaks down text into subword tokens.\n",
    "The code I wrote to process text, and the statistics it produces.\n",
    "\n",
    "## Why Byte Pair Encoding?\n",
    "\n",
    "Tokenization is the first step for any language model, transforming raw text into “tokens” that the model can understand. For example, simple tokenizers may treat each word as a token or break it down further into characters. However, this approach doesn’t handle out-of-vocabulary words well and can inflate the number of tokens, especially for rare words.\n",
    "\n",
    "Here’s where BPE comes into play. BPE compresses the text by merging the most frequent pairs of characters or subwords iteratively. It effectively balances tokenization by handling common words as whole tokens while splitting rare words into subword tokens. This allows LLMs to process unknown words without overwhelming the model with too many individual tokens.\n",
    "\n",
    "## The Importance of Avoiding UNK Tokens\n",
    "\n",
    "When building language models, encountering unknown tokens (UNK) can be detrimental to performance. An UNK token represents any word that is not present in the model’s vocabulary, and while it’s a common fallback in traditional tokenization approaches, it introduces several significant drawbacks:\n",
    "\n",
    "- Loss of Information: When a model encounters an UNK token, it loses all information about that word. This means that any context, meaning, or nuance associated with that word is entirely discarded.\n",
    "- Inefficiency in Language Representation: UNK tokens can lead to inefficient representations of language. When a significant portion of the text consists of UNK tokens, the model is forced to work with a distorted understanding of the input.\n",
    "- Inability to Generalize: The presence of UNK tokens hampers a model’s ability to generalize from training data to unseen text. If the model has not learned to associate certain words or phrases with specific contexts, it may struggle with understanding similar phrases or variations.\n",
    "- Impact on Downstream Tasks: For tasks such as translation or summarization, encountering UNK tokens can severely degrade performance, leading to misinterpretations of user intent.\n",
    "\n",
    "## How BPE Mitigates the UNK Token Problem\n",
    "\n",
    "Byte Pair Encoding (BPE) addresses the issue of UNK tokens effectively by breaking down words into smaller, more manageable subword units. Here’s how BPE helps:\n",
    "\n",
    "- Subword Representation: By merging the most frequent pairs of characters or subwords iteratively, BPE allows for the representation of less frequent words as combinations of known subwords.\n",
    "- Flexibility in Vocabulary: BPE expands the vocabulary in a way that allows for greater coverage of the language without excessively increasing the number of tokens.\n",
    "- Improved Contextual Understanding: When words are tokenized into subwords, the model retains more information about the input text.\n",
    "- Better Generalization: With BPE, models can generalize better to unseen words or phrases.\n",
    "- A Simple Byte Pair Encoding (BPE) Implementation\n",
    "\n",
    "In this simplified approach, I wrote code to tokenize text into subword units. It reads a .txt file, splits words into tokens of 2-3 characters, and ensures that punctuation and spaces are treated as separate tokens. Here’s the key structure of the tokenizer:\n",
    "\n",
    "- Text Normalization: Convert all characters to lowercase to ensure uniformity.\n",
    "- Word Splitting: Split words longer than 3 characters into 2- or 3-character tokens.\n",
    "- Punctuation Handling: Treat each punctuation and space as individual tokens.\n",
    "\n",
    "Here’s a glance at the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Output (TokenID: Token):\n",
      "65: in\n",
      "59: a\n",
      "103: qui\n",
      "377: et\n",
      "150: vil\n",
      "2: lag\n",
      "51: e\n",
      "369: nes\n",
      "129: tle\n",
      "5: d\n",
      "130: bet\n",
      "357: wee\n",
      "118: n\n",
      "162: the\n",
      "133: mou\n",
      "266: nta\n",
      "154: ins\n",
      "229: and\n",
      "162: the\n",
      "169: sea\n",
      "206: ,\n",
      "162: the\n",
      "339: peo\n",
      "111: ple\n",
      "8: liv\n",
      "314: ed\n",
      "220: pea\n",
      "335: cef\n",
      "404: ull\n",
      "329: y\n",
      "67: for\n",
      "364: gen\n",
      "274: era\n",
      "6: tio\n",
      "351: ns\n",
      "77: .\n",
      "162: the\n",
      "40: air\n",
      "95: was\n",
      "363: cri\n",
      "395: sp\n",
      "206: ,\n",
      "162: the\n",
      "155: wat\n",
      "29: ers\n",
      "174: cle\n",
      "358: ar\n",
      "206: ,\n",
      "229: and\n",
      "182: tim\n",
      "51: e\n",
      "242: see\n",
      "11: med\n",
      "346: to\n",
      "79: slo\n",
      "19: w\n",
      "143: dow\n",
      "118: n\n",
      "108: as\n",
      "378: one\n",
      "4: str\n",
      "137: oll\n",
      "314: ed\n",
      "152: thr\n",
      "98: oug\n",
      "342: h\n",
      "162: the\n",
      "241: cob\n",
      "46: ble\n",
      "285: sto\n",
      "295: ne\n",
      "4: str\n",
      "414: eet\n",
      "338: s\n",
      "77: .\n",
      "197: eve\n",
      "94: ry\n",
      "61: mor\n",
      "302: nin\n",
      "343: g\n",
      "206: ,\n",
      "162: the\n",
      "181: bak\n",
      "101: ery\n",
      "148: on\n",
      "162: the\n",
      "275: cor\n",
      "107: ner\n",
      "117: wou\n",
      "128: ld\n",
      "9: fil\n",
      "165: l\n",
      "162: the\n",
      "40: air\n",
      "22: wit\n",
      "342: h\n",
      "162: the\n",
      "386: sce\n",
      "355: nt\n",
      "315: of\n",
      "391: fre\n",
      "376: sh\n",
      "15: bre\n",
      "193: ad\n",
      "206: ,\n",
      "306: lur\n",
      "185: ing\n",
      "146: bot\n",
      "342: h\n",
      "264: loc\n",
      "310: als\n",
      "229: and\n",
      "30: vis\n",
      "120: ito\n",
      "262: rs\n",
      "299: ali\n",
      "244: ke\n",
      "77: .\n",
      "84: how\n",
      "197: eve\n",
      "39: r\n",
      "206: ,\n",
      "198: not\n",
      "313: all\n",
      "95: was\n",
      "108: as\n",
      "211: it\n",
      "242: see\n",
      "11: med\n",
      "77: .\n",
      "298: ben\n",
      "240: eat\n",
      "342: h\n",
      "237: thi\n",
      "338: s\n",
      "220: pea\n",
      "335: cef\n",
      "41: ul\n",
      "347: ext\n",
      "99: eri\n",
      "216: or\n",
      "206: ,\n",
      "59: a\n",
      "50: ser\n",
      "259: ies\n",
      "315: of\n",
      "4: str\n",
      "290: ang\n",
      "51: e\n",
      "92: occ\n",
      "320: urr\n",
      "24: enc\n",
      "311: es\n",
      "297: beg\n",
      "322: an\n",
      "346: to\n",
      "271: uns\n",
      "380: ett\n",
      "167: le\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "29: ers\n",
      "77: .\n",
      "123: fir\n",
      "201: st\n",
      "206: ,\n",
      "162: the\n",
      "8: liv\n",
      "106: est\n",
      "60: ock\n",
      "260: sta\n",
      "139: rte\n",
      "5: d\n",
      "346: to\n",
      "243: van\n",
      "280: ish\n",
      "22: wit\n",
      "218: hou\n",
      "14: t\n",
      "59: a\n",
      "48: tra\n",
      "278: ce\n",
      "77: .\n",
      "162: the\n",
      "118: n\n",
      "206: ,\n",
      "4: str\n",
      "290: ang\n",
      "51: e\n",
      "325: sym\n",
      "330: bol\n",
      "338: s\n",
      "252: wer\n",
      "51: e\n",
      "390: fou\n",
      "151: nd\n",
      "212: etc\n",
      "100: hed\n",
      "34: int\n",
      "389: o\n",
      "162: the\n",
      "203: wal\n",
      "405: ls\n",
      "315: of\n",
      "291: old\n",
      "308: bui\n",
      "184: ldi\n",
      "398: ngs\n",
      "77: .\n",
      "267: at\n",
      "411: nig\n",
      "340: ht\n",
      "206: ,\n",
      "162: the\n",
      "160: sou\n",
      "151: nd\n",
      "315: of\n",
      "124: dis\n",
      "91: tan\n",
      "14: t\n",
      "16: wai\n",
      "221: lin\n",
      "343: g\n",
      "226: cou\n",
      "128: ld\n",
      "366: be\n",
      "307: hea\n",
      "28: rd\n",
      "93: ech\n",
      "215: oin\n",
      "343: g\n",
      "63: fro\n",
      "256: m\n",
      "162: the\n",
      "67: for\n",
      "106: est\n",
      "77: .\n",
      "162: the\n",
      "250: tow\n",
      "365: nsf\n",
      "86: olk\n",
      "252: wer\n",
      "51: e\n",
      "26: baf\n",
      "109: fle\n",
      "5: d\n",
      "77: .\n",
      "337: som\n",
      "51: e\n",
      "401: cla\n",
      "104: ime\n",
      "5: d\n",
      "211: it\n",
      "95: was\n",
      "162: the\n",
      "234: wor\n",
      "119: k\n",
      "315: of\n",
      "246: anc\n",
      "136: ien\n",
      "14: t\n",
      "304: spi\n",
      "36: rit\n",
      "338: s\n",
      "206: ,\n",
      "350: oth\n",
      "29: ers\n",
      "402: whi\n",
      "316: spe\n",
      "147: red\n",
      "247: abo\n",
      "135: ut\n",
      "59: a\n",
      "227: sec\n",
      "231: ret\n",
      "72: cul\n",
      "14: t\n",
      "361: tha\n",
      "14: t\n",
      "205: had\n",
      "317: tak\n",
      "115: en\n",
      "54: roo\n",
      "14: t\n",
      "224: nea\n",
      "1: rby\n",
      "77: .\n",
      "407: des\n",
      "257: pit\n",
      "51: e\n",
      "162: the\n",
      "210: gro\n",
      "173: win\n",
      "343: g\n",
      "255: ten\n",
      "276: sio\n",
      "118: n\n",
      "206: ,\n",
      "56: lif\n",
      "51: e\n",
      "219: wen\n",
      "14: t\n",
      "148: on\n",
      "77: .\n",
      "412: chi\n",
      "186: ldr\n",
      "115: en\n",
      "269: sti\n",
      "413: ll\n",
      "168: pla\n",
      "178: yed\n",
      "65: in\n",
      "162: the\n",
      "4: str\n",
      "414: eet\n",
      "338: s\n",
      "206: ,\n",
      "83: mer\n",
      "265: cha\n",
      "239: nts\n",
      "233: set\n",
      "80: up\n",
      "162: the\n",
      "164: ir\n",
      "260: sta\n",
      "251: lls\n",
      "206: ,\n",
      "229: and\n",
      "12: fis\n",
      "287: her\n",
      "356: men\n",
      "213: cas\n",
      "14: t\n",
      "162: the\n",
      "164: ir\n",
      "294: net\n",
      "338: s\n",
      "34: int\n",
      "389: o\n",
      "162: the\n",
      "169: sea\n",
      "77: .\n",
      "378: one\n",
      "75: day\n",
      "206: ,\n",
      "59: a\n",
      "4: str\n",
      "290: ang\n",
      "284: er\n",
      "112: arr\n",
      "368: ive\n",
      "5: d\n",
      "267: at\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "51: e\n",
      "77: .\n",
      "392: he\n",
      "95: was\n",
      "367: tal\n",
      "165: l\n",
      "206: ,\n",
      "22: wit\n",
      "342: h\n",
      "64: pie\n",
      "166: rci\n",
      "176: ng\n",
      "194: blu\n",
      "51: e\n",
      "223: eye\n",
      "338: s\n",
      "229: and\n",
      "59: a\n",
      "281: sca\n",
      "39: r\n",
      "361: tha\n",
      "14: t\n",
      "382: ran\n",
      "63: fro\n",
      "256: m\n",
      "303: his\n",
      "78: che\n",
      "68: ek\n",
      "346: to\n",
      "303: his\n",
      "412: chi\n",
      "118: n\n",
      "77: .\n",
      "392: he\n",
      "400: spo\n",
      "244: ke\n",
      "248: lit\n",
      "129: tle\n",
      "206: ,\n",
      "187: but\n",
      "303: his\n",
      "207: pre\n",
      "158: sen\n",
      "278: ce\n",
      "95: was\n",
      "288: com\n",
      "18: man\n",
      "344: din\n",
      "343: g\n",
      "77: .\n",
      "332: rum\n",
      "7: ors\n",
      "228: spr\n",
      "43: ead\n",
      "103: qui\n",
      "301: ckl\n",
      "329: y\n",
      "247: abo\n",
      "135: ut\n",
      "254: who\n",
      "392: he\n",
      "95: was\n",
      "77: .\n",
      "337: som\n",
      "51: e\n",
      "49: sai\n",
      "5: d\n",
      "392: he\n",
      "95: was\n",
      "59: a\n",
      "48: tra\n",
      "360: vel\n",
      "284: er\n",
      "63: fro\n",
      "256: m\n",
      "59: a\n",
      "124: dis\n",
      "91: tan\n",
      "14: t\n",
      "191: lan\n",
      "5: d\n",
      "206: ,\n",
      "350: oth\n",
      "29: ers\n",
      "214: bel\n",
      "171: iev\n",
      "314: ed\n",
      "392: he\n",
      "95: was\n",
      "59: a\n",
      "172: sol\n",
      "336: die\n",
      "39: r\n",
      "148: on\n",
      "162: the\n",
      "319: run\n",
      "77: .\n",
      "113: wha\n",
      "379: tev\n",
      "284: er\n",
      "162: the\n",
      "213: cas\n",
      "51: e\n",
      "206: ,\n",
      "303: his\n",
      "112: arr\n",
      "134: iva\n",
      "165: l\n",
      "352: coi\n",
      "383: nci\n",
      "273: ded\n",
      "22: wit\n",
      "342: h\n",
      "322: an\n",
      "145: esc\n",
      "282: ala\n",
      "6: tio\n",
      "118: n\n",
      "315: of\n",
      "162: the\n",
      "4: str\n",
      "290: ang\n",
      "51: e\n",
      "197: eve\n",
      "239: nts\n",
      "77: .\n",
      "361: tha\n",
      "14: t\n",
      "411: nig\n",
      "340: ht\n",
      "206: ,\n",
      "108: as\n",
      "162: the\n",
      "114: moo\n",
      "118: n\n",
      "17: hun\n",
      "343: g\n",
      "394: ful\n",
      "165: l\n",
      "229: and\n",
      "253: low\n",
      "65: in\n",
      "162: the\n",
      "10: sky\n",
      "206: ,\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "51: e\n",
      "95: was\n",
      "331: sha\n",
      "73: ken\n",
      "387: by\n",
      "322: an\n",
      "62: ear\n",
      "200: thq\n",
      "328: uak\n",
      "51: e\n",
      "77: .\n",
      "211: it\n",
      "95: was\n",
      "118: n\n",
      "126: ’\n",
      "14: t\n",
      "4: str\n",
      "375: ong\n",
      "354: eno\n",
      "321: ugh\n",
      "346: to\n",
      "76: cau\n",
      "258: se\n",
      "50: ser\n",
      "131: iou\n",
      "338: s\n",
      "140: dam\n",
      "27: age\n",
      "206: ,\n",
      "187: but\n",
      "211: it\n",
      "95: was\n",
      "354: eno\n",
      "321: ugh\n",
      "346: to\n",
      "263: wak\n",
      "51: e\n",
      "197: eve\n",
      "20: ryo\n",
      "295: ne\n",
      "63: fro\n",
      "256: m\n",
      "162: the\n",
      "164: ir\n",
      "38: bed\n",
      "338: s\n",
      "77: .\n",
      "162: the\n",
      "4: str\n",
      "290: ang\n",
      "284: er\n",
      "206: ,\n",
      "84: how\n",
      "197: eve\n",
      "39: r\n",
      "206: ,\n",
      "95: was\n",
      "3: now\n",
      "287: her\n",
      "51: e\n",
      "346: to\n",
      "366: be\n",
      "390: fou\n",
      "151: nd\n",
      "77: .\n",
      "169: sea\n",
      "81: rch\n",
      "217: par\n",
      "300: tie\n",
      "338: s\n",
      "252: wer\n",
      "51: e\n",
      "67: for\n",
      "11: med\n",
      "206: ,\n",
      "187: but\n",
      "162: the\n",
      "329: y\n",
      "408: yie\n",
      "192: lde\n",
      "5: d\n",
      "37: no\n",
      "349: res\n",
      "261: ult\n",
      "338: s\n",
      "77: .\n",
      "162: the\n",
      "67: for\n",
      "106: est\n",
      "66: loo\n",
      "11: med\n",
      "277: lar\n",
      "341: ge\n",
      "229: and\n",
      "67: for\n",
      "309: ebo\n",
      "344: din\n",
      "343: g\n",
      "148: on\n",
      "162: the\n",
      "222: out\n",
      "23: ski\n",
      "199: rts\n",
      "206: ,\n",
      "229: and\n",
      "326: few\n",
      "393: dar\n",
      "314: ed\n",
      "346: to\n",
      "85: ven\n",
      "87: tur\n",
      "51: e\n",
      "403: too\n",
      "396: dee\n",
      "90: p\n",
      "34: int\n",
      "389: o\n",
      "204: its\n",
      "331: sha\n",
      "143: dow\n",
      "338: s\n",
      "77: .\n",
      "357: wee\n",
      "153: ks\n",
      "183: pas\n",
      "21: sed\n",
      "206: ,\n",
      "229: and\n",
      "162: the\n",
      "324: une\n",
      "47: ase\n",
      "388: onl\n",
      "329: y\n",
      "88: gre\n",
      "19: w\n",
      "77: .\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "29: ers\n",
      "297: beg\n",
      "322: an\n",
      "346: to\n",
      "122: los\n",
      "51: e\n",
      "202: hop\n",
      "51: e\n",
      "206: ,\n",
      "337: som\n",
      "51: e\n",
      "109: fle\n",
      "127: ein\n",
      "343: g\n",
      "346: to\n",
      "190: nei\n",
      "52: ghb\n",
      "132: ori\n",
      "176: ng\n",
      "250: tow\n",
      "351: ns\n",
      "65: in\n",
      "169: sea\n",
      "81: rch\n",
      "315: of\n",
      "116: saf\n",
      "372: ety\n",
      "77: .\n",
      "362: yet\n",
      "206: ,\n",
      "350: oth\n",
      "29: ers\n",
      "58: rem\n",
      "89: ain\n",
      "314: ed\n",
      "206: ,\n",
      "353: det\n",
      "42: erm\n",
      "97: ine\n",
      "5: d\n",
      "346: to\n",
      "82: unc\n",
      "410: ove\n",
      "39: r\n",
      "162: the\n",
      "159: tru\n",
      "189: th\n",
      "77: .\n",
      "211: it\n",
      "95: was\n",
      "292: dur\n",
      "185: ing\n",
      "237: thi\n",
      "338: s\n",
      "182: tim\n",
      "51: e\n",
      "361: tha\n",
      "14: t\n",
      "322: an\n",
      "291: old\n",
      "235: wom\n",
      "322: an\n",
      "206: ,\n",
      "378: one\n",
      "315: of\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "51: e\n",
      "296: eld\n",
      "29: ers\n",
      "206: ,\n",
      "297: beg\n",
      "322: an\n",
      "346: to\n",
      "305: tel\n",
      "165: l\n",
      "285: sto\n",
      "249: rie\n",
      "338: s\n",
      "315: of\n",
      "59: a\n",
      "209: hid\n",
      "208: den\n",
      "55: pow\n",
      "284: er\n",
      "161: bur\n",
      "238: ied\n",
      "396: dee\n",
      "90: p\n",
      "22: wit\n",
      "236: hin\n",
      "162: the\n",
      "191: lan\n",
      "5: d\n",
      "77: .\n",
      "57: \"\n",
      "211: it\n",
      "126: ’\n",
      "338: s\n",
      "198: not\n",
      "312: jus\n",
      "14: t\n",
      "162: the\n",
      "67: for\n",
      "106: est\n",
      "206: ,\n",
      "57: \"\n",
      "138: she\n",
      "402: whi\n",
      "316: spe\n",
      "147: red\n",
      "206: ,\n",
      "287: her\n",
      "44: voi\n",
      "278: ce\n",
      "105: tre\n",
      "125: mbl\n",
      "185: ing\n",
      "22: wit\n",
      "342: h\n",
      "27: age\n",
      "206: ,\n",
      "57: \"\n",
      "162: the\n",
      "188: re\n",
      "163: are\n",
      "67: for\n",
      "327: ces\n",
      "287: her\n",
      "51: e\n",
      "291: old\n",
      "284: er\n",
      "361: tha\n",
      "118: n\n",
      "182: tim\n",
      "51: e\n",
      "204: its\n",
      "69: elf\n",
      "77: .\n",
      "57: \"\n",
      "373: cur\n",
      "409: ios\n",
      "232: ity\n",
      "272: led\n",
      "59: a\n",
      "141: sma\n",
      "413: ll\n",
      "210: gro\n",
      "80: up\n",
      "315: of\n",
      "150: vil\n",
      "2: lag\n",
      "29: ers\n",
      "346: to\n",
      "268: inv\n",
      "106: est\n",
      "156: iga\n",
      "289: te\n",
      "162: the\n",
      "291: old\n",
      "235: wom\n",
      "322: an\n",
      "126: ’\n",
      "338: s\n",
      "401: cla\n",
      "74: ims\n",
      "77: .\n",
      "162: the\n",
      "329: y\n",
      "359: gat\n",
      "287: her\n",
      "314: ed\n",
      "71: sup\n",
      "196: pli\n",
      "311: es\n",
      "229: and\n",
      "233: set\n",
      "374: off\n",
      "34: int\n",
      "389: o\n",
      "162: the\n",
      "32: woo\n",
      "385: ds\n",
      "77: .\n",
      "162: the\n",
      "396: dee\n",
      "170: per\n",
      "162: the\n",
      "329: y\n",
      "219: wen\n",
      "14: t\n",
      "206: ,\n",
      "162: the\n",
      "61: mor\n",
      "51: e\n",
      "162: the\n",
      "329: y\n",
      "270: fel\n",
      "14: t\n",
      "59: a\n",
      "4: str\n",
      "290: ang\n",
      "51: e\n",
      "179: ene\n",
      "102: rgy\n",
      "65: in\n",
      "162: the\n",
      "40: air\n",
      "77: .\n",
      "211: it\n",
      "95: was\n",
      "108: as\n",
      "175: if\n",
      "162: the\n",
      "67: for\n",
      "106: est\n",
      "95: was\n",
      "299: ali\n",
      "45: ve\n",
      "206: ,\n",
      "155: wat\n",
      "412: chi\n",
      "176: ng\n",
      "162: the\n",
      "256: m\n",
      "206: ,\n",
      "16: wai\n",
      "318: tin\n",
      "343: g\n",
      "77: .\n",
      "197: eve\n",
      "225: ntu\n",
      "313: all\n",
      "329: y\n",
      "206: ,\n",
      "162: the\n",
      "329: y\n",
      "142: cam\n",
      "51: e\n",
      "230: upo\n",
      "118: n\n",
      "59: a\n",
      "174: cle\n",
      "31: ari\n",
      "176: ng\n",
      "77: .\n",
      "65: in\n",
      "162: the\n",
      "180: cen\n",
      "53: ter\n",
      "285: sto\n",
      "110: od\n",
      "59: a\n",
      "277: lar\n",
      "341: ge\n",
      "285: sto\n",
      "295: ne\n",
      "206: ,\n",
      "283: cov\n",
      "323: ere\n",
      "5: d\n",
      "65: in\n",
      "162: the\n",
      "371: sam\n",
      "51: e\n",
      "325: sym\n",
      "330: bol\n",
      "338: s\n",
      "361: tha\n",
      "14: t\n",
      "205: had\n",
      "13: app\n",
      "62: ear\n",
      "314: ed\n",
      "65: in\n",
      "162: the\n",
      "150: vil\n",
      "2: lag\n",
      "51: e\n",
      "77: .\n",
      "187: but\n",
      "35: bef\n",
      "144: ore\n",
      "162: the\n",
      "329: y\n",
      "226: cou\n",
      "128: ld\n",
      "268: inv\n",
      "106: est\n",
      "156: iga\n",
      "289: te\n",
      "121: fur\n",
      "162: the\n",
      "39: r\n",
      "206: ,\n",
      "59: a\n",
      "237: thi\n",
      "279: ck\n",
      "345: mis\n",
      "14: t\n",
      "245: rol\n",
      "272: led\n",
      "65: in\n",
      "206: ,\n",
      "293: obs\n",
      "373: cur\n",
      "185: ing\n",
      "162: the\n",
      "164: ir\n",
      "406: vie\n",
      "19: w\n",
      "229: and\n",
      "397: sil\n",
      "24: enc\n",
      "185: ing\n",
      "162: the\n",
      "234: wor\n",
      "128: ld\n",
      "381: aro\n",
      "333: und\n",
      "162: the\n",
      "256: m\n",
      "77: .\n",
      "195: sud\n",
      "208: den\n",
      "157: ly\n",
      "206: ,\n",
      "162: the\n",
      "329: y\n",
      "307: hea\n",
      "28: rd\n",
      "59: a\n",
      "44: voi\n",
      "278: ce\n",
      "77: .\n",
      "211: it\n",
      "95: was\n",
      "118: n\n",
      "126: ’\n",
      "14: t\n",
      "286: lou\n",
      "5: d\n",
      "206: ,\n",
      "187: but\n",
      "211: it\n",
      "93: ech\n",
      "96: oed\n",
      "65: in\n",
      "162: the\n",
      "164: ir\n",
      "149: min\n",
      "385: ds\n",
      "108: as\n",
      "175: if\n",
      "211: it\n",
      "205: had\n",
      "25: alw\n",
      "348: ays\n",
      "70: bee\n",
      "118: n\n",
      "162: the\n",
      "188: re\n",
      "77: .\n",
      "33: “\n",
      "384: you\n",
      "334: sho\n",
      "399: uld\n",
      "118: n\n",
      "126: ’\n",
      "14: t\n",
      "177: hav\n",
      "51: e\n",
      "288: com\n",
      "51: e\n",
      "206: ,\n",
      "370: ”\n",
      "211: it\n",
      "49: sai\n",
      "5: d\n",
      "77: .\n",
      "\n",
      "Top 20 Tokens by Frequency:\n",
      "Token ID   Token           Frequency \n",
      "-----------------------------------\n",
      "162        the             64        \n",
      "206        ,               45        \n",
      "77         .               38        \n",
      "51         e               34        \n",
      "14         t               20        \n",
      "338        s               17        \n",
      "59         a               16        \n",
      "95         was             16        \n",
      "5          d               13        \n",
      "118        n               13        \n",
      "346        to              13        \n",
      "229        and             11        \n",
      "329        y               11        \n",
      "211        it              11        \n",
      "65         in              10        \n",
      "4          str             10        \n",
      "315        of              10        \n",
      "343        g               9         \n",
      "150        vil             8         \n",
      "2          lag             8         \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, text=None):\n",
    "        \"\"\"Initialize the BPE Tokenizer with optional text.\"\"\"\n",
    "        self.text = text\n",
    "        self.token_to_id = {}\n",
    "        self.token_frequencies = Counter()\n",
    "\n",
    "    def normalize_text(self):\n",
    "        \"\"\"Normalize the input text to lowercase.\"\"\"\n",
    "        if self.text is not None:\n",
    "            self.text = self.text.lower()\n",
    "\n",
    "    def tokenize(self):\n",
    "        \"\"\"Tokenize the text into words and punctuation.\"\"\"\n",
    "        return re.findall(r'\\w+|[^\\w\\s]', self.text, re.UNICODE)\n",
    "\n",
    "    def split_word_tokens(self, tokens):\n",
    "        \"\"\"Split words longer than 3 characters into 2- or 3-character tokens.\"\"\"\n",
    "        split_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token.isalpha():  # Split only alphabetic tokens\n",
    "                if len(token) <= 3:\n",
    "                    split_tokens.append(token)\n",
    "                else:\n",
    "                    # Split words into chunks of 2 or 3 characters\n",
    "                    i = 0\n",
    "                    while i < len(token):\n",
    "                        if i + 3 <= len(token):\n",
    "                            split_tokens.append(token[i:i + 3])\n",
    "                            i += 3\n",
    "                        else:\n",
    "                            split_tokens.append(token[i:i + 2])\n",
    "                            i += 2\n",
    "            else:\n",
    "                split_tokens.append(token)  # Keep punctuation unchanged\n",
    "\n",
    "        return split_tokens\n",
    "\n",
    "    def assign_token_ids(self, tokens):\n",
    "        \"\"\"Assign token IDs and count frequencies.\"\"\"\n",
    "        unique_tokens = list(set(tokens))\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(unique_tokens, 1)}  # Start IDs from 1\n",
    "        \n",
    "        # Count the frequency of each token\n",
    "        self.token_frequencies = Counter(tokens)\n",
    "\n",
    "    def display_top_tokens(self, n=20):\n",
    "        \"\"\"Display the top n tokens by frequency.\"\"\"\n",
    "        top_tokens = self.token_frequencies.most_common(n)\n",
    "        \n",
    "        print(f\"{'Token ID':<10} {'Token':<15} {'Frequency':<10}\")\n",
    "        print('-' * 35)\n",
    "        for token, freq in top_tokens:\n",
    "            print(f\"{self.token_to_id[token]:<10} {token:<15} {freq:<10}\")\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"Main processing function to normalize, tokenize, and assign IDs.\"\"\"\n",
    "        self.normalize_text()\n",
    "        tokens = self.tokenize()\n",
    "        split_tokens = self.split_word_tokens(tokens)\n",
    "        self.assign_token_ids(split_tokens)\n",
    "        return split_tokens\n",
    "\n",
    "\n",
    "def read_input_file(file_path):\n",
    "    \"\"\"Read input text from a .txt file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "# Test the BPE tokenizer with a .txt file as input\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify the path to your input file\n",
    "    file_path = \"data.txt\"  # Replace with your actual file path\n",
    "    \n",
    "    # Step 1: Read the input file\n",
    "    input_text = read_input_file(file_path)\n",
    "\n",
    "    # Step 2: Create an instance of the BPETokenizer\n",
    "    tokenizer = BPETokenizer(input_text)\n",
    "\n",
    "    # Step 3: Process the text and display results\n",
    "    final_tokens = tokenizer.process()\n",
    "    \n",
    "    print(\"\\nTokenized Output (TokenID: Token):\")\n",
    "    for token in final_tokens:\n",
    "        print(f\"{tokenizer.token_to_id[token]}: {token}\")\n",
    "    \n",
    "    # Step 4: Display the top 20 tokens by frequency\n",
    "    print(\"\\nTop 20 Tokens by Frequency:\")\n",
    "    tokenizer.display_top_tokens()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Example\n",
    "After running the tokenizer on a sample text file, here’s what the tokenized output looks like:\n",
    "\n",
    "```\n",
    "Tokenized Output (TokenID: Token):\n",
    "65: in\n",
    "59: a\n",
    "103: qui\n",
    "377: et\n",
    "150: vil\n",
    "2: lag\n",
    "51: e\n",
    "369: nes\n",
    "129: tle\n",
    "5: d\n",
    "130: bet\n",
    "357: weepy\n",
    "```\n",
    "\n",
    "Each word is split into subwords of 2–3 characters, and tokens like punctuation marks and spaces remain as individual tokens\n",
    "\n",
    "## Token Statistics: Understanding the Distribution\n",
    "\n",
    "After processing the text, I implemented a simple function to display the most frequent tokens. This allows us to gain insights into which subwords or punctuation marks are common in the dataset.\n",
    "\n",
    "Here’s an example of the Top 20 tokens by frequency:\n",
    "\n",
    "```\n",
    "Top 20 Tokens by Frequency:\n",
    "Token ID   Token           Frequency \n",
    "-----------------------------------\n",
    "162        the             64        \n",
    "206        ,               45        \n",
    "77         .               38        \n",
    "51         e               34        \n",
    "14         t               20        \n",
    "338        s               17        \n",
    "59         a               16        \n",
    "95         was             16        \n",
    "5          d               13        \n",
    "118        n               13        \n",
    "346        to              13        \n",
    "229        and             11        \n",
    "329        y               11        \n",
    "211        it              11        \n",
    "65         in              10        \n",
    "4          str             10        \n",
    "315        of              10        \n",
    "343        g               9         \n",
    "150        vil             8         \n",
    "2          lag             8 \n",
    "```\n",
    "\n",
    "We can see that “the” appears most frequently, which is expected for English text. Other common tokens include punctuation marks like commas and periods. By splitting words into manageable subword units, the tokenizer can handle both common and rare words efficiently.\n",
    "\n",
    "## Byte Pair Encoding (BPE) in Action\n",
    "\n",
    "The implementation showcased above is just a dummy example of BPE. A full-fledged BPE tokenizer would include multiple iterations of merging the most frequent pairs of characters or subwords in the vocabulary. Here’s a breakdown of how BPE works in practice:\n",
    "\n",
    "- Initial Vocabulary: Start with individual characters.\n",
    "- Pair Merging: In each iteration, merge the most frequent pair of adjacent characters or subwords.\n",
    "- Token Creation: As pairs are merged, the vocabulary expands, allowing the model to represent common words as single tokens while breaking down rarer words into smaller units.\n",
    "\n",
    "This approach allows LLMs to efficiently encode the text, minimizing the total number of tokens without sacrificing the model’s ability to generalize to new or rare words.\n",
    "\n",
    "## Extending the Concept\n",
    "\n",
    "Although my implementation is a simplified example, BPE is widely used in models like GPT-3 and BERT. In those models, BPE allows for highly flexible tokenization, where even unfamiliar words are tokenized into manageable subwords rather than being treated as out-of-vocabulary (OOV) words.\n",
    "\n",
    "In real-world applications, this leads to models that can process any text efficiently, even if the exact words or phrases were not present in the training data. This feature makes BPE-based tokenizers a crucial part of modern NLP architectures.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Byte Pair Encoding is essential for making large language models more efficient and flexible in handling diverse text. While this implementation is just a starting point, it provides a clear illustration of how BPE can split words into subwords, keeping the token count manageable while ensuring no word is left behind.\n",
    "\n",
    "By experimenting with BPE, I’ve gained a deeper understanding of tokenization — a crucial step that allows modern LLMs to process massive amounts of text with ease.\n",
    "\n",
    "Try it out! You can use the provided code to tokenize your own text and explore how different subwords and tokens are represented. Whether you’re building a full-fledged LLM or just learning about natural language processing, implementing a tokenizer is a great way to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
