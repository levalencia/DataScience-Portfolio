{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Large Language Models Use Sliding Windows for Next-Word Prediction — With PyTorch\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this post, we’ll dive into how Large Language Models (LLMs) predict the next word in a sequence using the sliding window technique. This simple yet powerful concept forms the basis of training LLMs like GPT-3.\n",
    "\n",
    "We’ll explain:\n",
    "\n",
    "- How to tokenize text and convert it into a form that models can learn from.\n",
    "- How to implement the sliding window approach using PyTorch’s Dataset class.\n",
    "- How to structure the solution across multiple Python files for clarity and maintainability.\n",
    "\n",
    "By the end of this post, you’ll have a solid understanding of how LLMs learn to predict text, along with a reusable code structure for further experiments.\n",
    "\n",
    "## Understanding the Sliding Window Approach and Next-Word Prediction\n",
    "\n",
    "Next-Word Prediction is the core task that LLMs are trained on. When a model generates text, it predicts what word (or token) comes next in a sentence based on the preceding context. For example, if the model sees the phrase “The cat sat on the,” it should predict that “mat” is likely to follow.\n",
    "\n",
    "The Sliding Window Approach is a method to create training examples for this task. Instead of feeding the model the entire sentence at once, we divide the text into smaller, overlapping chunks or “windows.” Each window contains a fixed number of tokens that serve as input, and the token immediately following the window is the target for prediction.\n",
    "\n",
    "Here’s how the sliding window works in practice:\n",
    "\n",
    "- Window Size: This is the number of tokens the model sees at one time. For instance, if the window size is 4, the model looks at four words.\n",
    "- Stride: This is how many tokens the window moves forward after each prediction. If the stride is 1, the model moves one token forward; if it’s 2, the window skips one token.\n",
    "Example: Given the sentence:\n",
    "\n",
    "```\n",
    "\"The quick brown fox jumps over the lazy dog.\"\n",
    "```\n",
    "With a window size of 4 and a stride of 1, the input-output pairs generated would be:\n",
    "\n",
    "```\n",
    "Input: [\"The\", \"quick\", \"brown\", \"fox\"] → Output: \"jumps\"\n",
    "Input: [\"quick\", \"brown\", \"fox\", \"jumps\"] → Output: \"over\"\n",
    "Input: [\"brown\", \"fox\", \"jumps\", \"over\"] → Output: \"the\"\n",
    "```\n",
    "\n",
    "This technique helps the model learn the relationships between words in a sentence, enabling it to generate coherent text.\n",
    "\n",
    "See this gif:\n",
    "\n",
    "![Alt text](token_prediction.gif \"Image Title\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Structuring the Code: Files and Organization\n",
    "To keep things clean and maintainable, we’ll break the code into three files:\n",
    "\n",
    "- tokenizer.py: For tokenizing the text and converting it to token IDs.\n",
    "- dataset.py: Defines the dataset class using the PyTorch Dataset base class.\n",
    "- main.py: The main script where we prepare the dataset and run the sliding window approach.\n",
    "Here’s how we’ll organize the project:\n",
    "\n",
    "```\n",
    "llm_sliding_window/\n",
    "│\n",
    "├── tokenizer.py\n",
    "├── dataset.py\n",
    "└── main.py\n",
    "```\n",
    "\n",
    "## 2. Tokenizing Text (tokenizer.py)\n",
    "LLMs don’t work directly with raw text — they convert words into tokens. In tokenizer.py, we’ll define a simple tokenizer that maps each unique word to a token ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.py\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Split text into words\n",
    "        self.words = text.split()\n",
    "        # Assign each unique word a token ID\n",
    "        self.tokens = {word: idx for idx, word in enumerate(set(self.words))}\n",
    "    \n",
    "    def tokenize(self):\n",
    "        \"\"\"Convert words to token IDs\"\"\"\n",
    "        return [self.tokens[word] for word in self.words]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"Convert token IDs back to words\"\"\"\n",
    "        reverse_tokens = {idx: word for word, idx in self.tokens.items()}\n",
    "        return [reverse_tokens[token] for token in token_ids]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\n",
    "    tokenizer = SimpleTokenizer(text)\n",
    "    token_ids = tokenizer.tokenize()\n",
    "    print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Custom Dataset (dataset.py)\n",
    "The heart of our solution is creating a custom dataset class that will generate input-output pairs (sliding windows) for training. We’ll inherit from PyTorch’s Dataset class to allow this dataset to be used with PyTorch’s data-loading and training tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    def __init__(self, tokenized_text, window_size, stride=1):\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.input_windows, self.output_tokens = self._generate_windows()\n",
    "    \n",
    "    def _generate_windows(self):\n",
    "        \"\"\"Generate input-output pairs using a sliding window approach.\"\"\"\n",
    "        input_windows = []\n",
    "        output_tokens = []\n",
    "        for i in range(0, len(self.tokenized_text) - self.window_size, self.stride):\n",
    "            # Input window of size `window_size`\n",
    "            input_windows.append(self.tokenized_text[i:i + self.window_size])\n",
    "            # The next token (the one we want to predict)\n",
    "            output_tokens.append(self.tokenized_text[i + self.window_size])\n",
    "        return input_windows, output_tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples.\"\"\"\n",
    "        return len(self.input_windows)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return a single input-output pair.\"\"\"\n",
    "        return torch.tensor(self.input_windows[idx]), torch.tensor(self.output_tokens[idx])\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    tokenized_text = [2, 4, 5, 6, 7, 1, 3, 8, 9, 10, 11, 12]  # Example tokenized text\n",
    "    dataset = SlidingWindowDataset(tokenized_text, window_size=4, stride=2)\n",
    "    for i in range(len(dataset)):\n",
    "        input_window, next_token = dataset[i]\n",
    "        print(f\"Input: {input_window}, Next token: {next_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bringing It All Together (main.py)\n",
    "Now that we have the SimpleTokenizer and SlidingWindowDataset ready, let’s use them together in the main.py file to simulate a real-world scenario. We’ll tokenize the text, create sliding windows, and print out the input-output pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "from tokenizer import SimpleTokenizer\n",
    "from dataset import SlidingWindowDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Sample text\n",
    "text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\n",
    "\n",
    "# Step 1: Tokenize the text\n",
    "tokenizer = SimpleTokenizer(text)\n",
    "tokenized_text = tokenizer.tokenize()\n",
    "print(\"Tokenized text:\", tokenized_text)\n",
    "\n",
    "# Step 2: Create the dataset\n",
    "window_size = 4\n",
    "stride = 1\n",
    "dataset = SlidingWindowDataset(tokenized_text, window_size=window_size, stride=stride)\n",
    "\n",
    "# Step 3: Load the data using PyTorch's DataLoader (batch_size=1 for simplicity)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Step 4: Iterate through the dataloader and print the input-output pairs\n",
    "print(\"\\nSliding Window Input-Output Pairs:\")\n",
    "for i, (input_window, next_token) in enumerate(dataloader):\n",
    "    decoded_input = tokenizer.decode(input_window.squeeze().tolist())\n",
    "    decoded_token = tokenizer.decode([next_token.item()])\n",
    "    print(f\"Sample {i+1}: Input: {decoded_input} -> Predict: {decoded_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Workflow\n",
    "\n",
    "- tokenizer.py: This file contains the SimpleTokenizer class, which splits the text into words, assigns token IDs to each unique word, and converts the text into a list of token IDs.\n",
    "- dataset.py: We define the SlidingWindowDataset, which generates sliding window input-output pairs. The dataset inherits from PyTorch’s Dataset class to make it compatible with PyTorch’s training ecosystem. This custom dataset can be easily expanded and reused.\n",
    "- main.py: This script brings everything together. It tokenizes the input text, creates sliding windows of token IDs, and uses PyTorch’s DataLoader to batch and iterate through the dataset. Each sliding window is shown along with the next token to predict.\n",
    "\n",
    "## 5. Running the Code\n",
    "\n",
    "Let’s now run the main.py file to see the sliding window technique in action. After tokenizing the text and generating sliding windows, the output will look like this:\n",
    "\n",
    "```\n",
    "Tokenized text: [2, 4, 5, 6, 7, 1, 3, 8, 9, 10, 11, 12]\n",
    "\n",
    "Sliding Window Input-Output Pairs:\n",
    "Sample 1: Input: ['Lorem', 'Ipsum', 'is', 'simply'] -> Predict: ['dummy']\n",
    "Sample 2: Input: ['Ipsum', 'is', 'simply', 'dummy'] -> Predict: ['text']\n",
    "Sample 3: Input: ['is', 'simply', 'dummy', 'text'] -> Predict: ['of']\n",
    "Sample 4: Input: ['simply', 'dummy', 'text', 'of'] -> Predict: ['the']\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model is trained to predict the next word based on the current context window.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial, we explored the sliding window technique used to train LLMs for next-word prediction. We built a simple but effective solution using PyTorch and organized our code into three separate files:\n",
    "\n",
    "- tokenizer.py for tokenization,\n",
    "- dataset.py for creating the sliding windows, and\n",
    "- main.py for tying everything together and iterating through the dataset.\n",
    "\n",
    "By following this approach, you can easily extend the solution to larger datasets, more complex tokenization schemes (like Byte Pair Encoding), and even start training your own language models!\n",
    "\n",
    "## Next Steps\n",
    "Expand the tokenizer to handle more complex inputs.\n",
    "Experiment with different window sizes and strides to see their impact on data sampling.\n",
    "Use this dataset with a neural network for next-token prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate animated gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation saved as sliding_window.gif\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def create_sliding_window_animation(input_text, window_size, stride, output_file='sliding_window.gif'):\n",
    "    # Set up the figure and axes\n",
    "    fig, (ax_text, ax_main) = plt.subplots(2, 1, figsize=(12, 6), height_ratios=[1, 3])\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    # Process input text\n",
    "    words = input_text.split()\n",
    "\n",
    "    # Initialize the main plot\n",
    "    ax_main.set_xlim(0, len(words))\n",
    "    ax_main.set_ylim(0, 1)\n",
    "    ax_main.set_xticks([])\n",
    "    ax_main.set_yticks([])\n",
    "\n",
    "    # Add the words to the main plot\n",
    "    for i, word in enumerate(words):\n",
    "        ax_main.text(i + 0.5, 0.5, word, ha='center', va='center', fontsize=10)\n",
    "\n",
    "    # Create the sliding window\n",
    "    window = patches.Rectangle((0, 0), window_size, 1, fill=False, edgecolor='red', lw=2)\n",
    "    ax_main.add_patch(window)\n",
    "\n",
    "    # Set up the text area\n",
    "    ax_text.axis('off')\n",
    "    title = ax_text.text(0.5, 0.7, \"Sliding Window in LLM Training\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    window_text = ax_text.text(0.5, 0.3, '', ha='center', va='center', fontsize=12)\n",
    "\n",
    "    def animate(frame):\n",
    "        i = frame * stride\n",
    "        window.set_x(i)\n",
    "        current_words = words[i:i+window_size]\n",
    "        window_text.set_text(f\"Current window: {' '.join(current_words)}\")\n",
    "        return window, window_text\n",
    "\n",
    "    # Calculate the number of frames based on stride\n",
    "    num_frames = (len(words) - window_size) // stride + 1\n",
    "\n",
    "    # Create the animation\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=num_frames, interval=1000, blit=True)\n",
    "\n",
    "    # Save the animation as a gif\n",
    "    anim.save(output_file, writer='pillow', fps=1)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Animation saved as {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\n",
    "create_sliding_window_animation(input_text=input_text, window_size=6, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animation saved as token_prediction.gif\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "def create_token_prediction_animation(tokens, words, window_size, output_file='token_prediction.gif'):\n",
    "    # Set up the figure and axes\n",
    "    fig, (ax_text, ax_main) = plt.subplots(2, 1, figsize=(12, 6), height_ratios=[1, 3])\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "\n",
    "    # Initialize the main plot\n",
    "    ax_main.set_xlim(0, len(tokens) + 1)\n",
    "    ax_main.set_ylim(0, 2)\n",
    "    ax_main.set_xticks([])\n",
    "    ax_main.set_yticks([])\n",
    "\n",
    "    # Add the tokens and words to the main plot\n",
    "    for i, (token, word) in enumerate(zip(tokens, words)):\n",
    "        ax_main.text(i + 0.5, 1.5, str(token), ha='center', va='center', fontsize=10, color='black')  # Token ID\n",
    "        ax_main.text(i + 0.5, 0.5, word, ha='center', va='center', fontsize=10, color='green')  # Word\n",
    "\n",
    "    # Create the sliding window for context tokens\n",
    "    window = patches.Rectangle((0, 0), window_size, 2, fill=False, edgecolor='red', lw=2)\n",
    "    ax_main.add_patch(window)\n",
    "\n",
    "    # Set up the text area\n",
    "    ax_text.axis('off')\n",
    "    title = ax_text.text(0.5, 0.7, \"LLM Predicting the Next Token\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "    window_text = ax_text.text(0.5, 0.3, '', ha='center', va='center', fontsize=12)\n",
    "\n",
    "    def animate(frame):\n",
    "        i = frame\n",
    "        current_tokens = tokens[i:i+window_size]\n",
    "        current_words = words[i:i+window_size]\n",
    "        next_token = tokens[i + window_size] if i + window_size < len(tokens) else None\n",
    "        next_word = words[i + window_size] if i + window_size < len(words) else None\n",
    "\n",
    "        # Update window position\n",
    "        window.set_x(i)\n",
    "\n",
    "        # Update the text to display the current context tokens and next predicted token\n",
    "        context_text = f\"Context tokens: {' '.join(map(str, current_tokens))}\"\n",
    "        context_words = f\"Context words: {' '.join(current_words)}\"\n",
    "        prediction_text = f\"Predicted next token: {next_token} ({next_word})\" if next_token is not None else \"End of sequence\"\n",
    "        window_text.set_text(f\"{context_text}\\n{context_words}\\n{prediction_text}\")\n",
    "\n",
    "        # Highlight the predicted token and word\n",
    "        ax_main.text(i + window_size + 0.5, 1.5, str(next_token), ha='center', va='center', fontsize=10, color='blue')\n",
    "        ax_main.text(i + window_size + 0.5, 0.5, next_word, ha='center', va='center', fontsize=10, color='blue')\n",
    "\n",
    "        return window, window_text\n",
    "\n",
    "    # Calculate the number of frames (we predict until the second-to-last token)\n",
    "    num_frames = len(tokens) - window_size\n",
    "\n",
    "    # Create the animation\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=num_frames, interval=1000, blit=True)\n",
    "\n",
    "    # Save the animation as a gif\n",
    "    anim.save(output_file, writer='pillow', fps=1)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Animation saved as {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "dummy_text = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry.\"\n",
    "words = dummy_text.split()  # List of words\n",
    "tokens = list(np.random.randint(1000, 1100, len(words)))  # Simulated token IDs for each word\n",
    "\n",
    "create_token_prediction_animation(tokens=tokens, words=words, window_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
