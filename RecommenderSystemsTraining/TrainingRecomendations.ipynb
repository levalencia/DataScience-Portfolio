{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Sytem for Trainings\n",
    "\n",
    "I guess you know Netflix, IMDB, Amazon and several other services, these services use extensively Recommendation Systems, they do it in order to encourage more use of their platforms or to increase the value of the items you are going to purchase.  In Netflix you get movie or series recommendations, in Amazon you get items recomendations that you might like.\n",
    "\n",
    "There is a lot of information there about Recomendation Systems, and I am not going to explain the theory, but if you are new, I encourage to read the following post for a theory introduction: https://towardsdatascience.com/introduction-to-recommender-systems-6c66cf15ada\n",
    "\n",
    "The idea of this project is easy, we have internal trainings created by our company employees, we have also external trainings that we take via pluralsight, udemy or any other platform like coursera, and we have employees which take those internal or external trainings.   Employees have some attributes, like department, language, skills, etc.    All those attributes need to be taken into account into our recommender system.\n",
    "\n",
    "For example:  if you are a new  employee with only 1 year of experience in Data Science (feature), and in the skills(feature) you have listed Statistics, Machine Learning, but another person in the company, has 10 years of experience in DataScience, with similar skills, and if that person took \"Advanced machine learning specialization\" in coursera.   The recommender system would be able to predict a.k.a recommend this training to the new employee.\n",
    "\n",
    "If you read the blog post above, you will see that some recommender systems only take into account the interaction between users and items, but not the feautures that describe the user and the items a.k.a metadata.  In this project I have used LightFM a very well known library in order to build a hybrid recommender system where the user features are also taken into account.\n",
    "\n",
    "To start, I will use 3 datasets:\n",
    "- Users Dataset with features like name, department, language, gender, etc.\n",
    "- Training Dataset with features like name, and main skill.\n",
    "- TrainingsTaken Dataset\n",
    "\n",
    "The last one is the relationship between a UserId and a Training ID, it's basically to know which user has taken which training. In recommender systems you can also have weights, a weight is basically a rating, in Movies, you can rate them 0–5 for example, so it's up to you to decide if you need weights or not for your business case. If you need weights, then you probably would put this field into the TrainingsTaken Dataset.\n",
    "\n",
    "I am working with Azure ML, so I registered the datasets in the ML Studio, you can see how I did this in my  post   [How to generate synthetic data with Faker in Python and Azure ML](https://python.plainenglish.io/how-to-generate-synthetic-data-with-faker-in-python-and-azure-ml-24f69ddaea0e \"How to generate synthetic data with Faker in Python and Azure ML\")\n",
    "\n",
    "With the code below, I am loading the datasets already registered from Azure ML into memory as pandas dataframe, this will allow us later to manipulate the data format as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "\n",
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = 'x'\n",
    "resource_group = 'y'\n",
    "workspace_name = 'z'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "datasetusers = Dataset.get_by_name(workspace, name='usersfake')\n",
    "usersdf = datasetusers.to_pandas_dataframe()\n",
    "\n",
    "datasettrainings = Dataset.get_by_name(workspace, name='trainings')\n",
    "trainingsdf = datasettrainings.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "datasettrainingstaken  = Dataset.get_by_name(workspace, name='trainingtakenfake')\n",
    "trainingstakendf = datasettrainingstaken.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check the contents of each dataframe, you can use df.head(5) and the output will be similar to the following images.\n",
    "\n",
    "![User Data Frame](https://miro.medium.com/max/700/1*-2utCuwrn560CeIqT6IoDg.png) \n",
    "User Data Frame\n",
    "\n",
    "![Training Dataframe](https://miro.medium.com/max/403/1*ceCmQcWiVWQ6CIN9v7qxwg.png) \n",
    "Traingn Data Frane\n",
    "\n",
    "![TrainingTaken Dataframe](https://miro.medium.com/max/291/1*BLQLhD89I_CV0HiwhbDz0w.png) \n",
    "Training Taken DataFrame\n",
    "\n",
    "Now we have 3 pandas dataframes that we can use with the LightFM algorithm.\n",
    "\n",
    "### LightFM\n",
    "\n",
    "LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It’s easy to use, fast (via multithreaded model estimation), and produces high-quality results. (source: https://github.com/lyst/lightfm)\n",
    "I selected LightFM because when searching for a hybrid recommender system, it was one of the most used ones, which allows user features and item features to be used within the model, however, there are other recommender systems you can try.\n",
    "To begin we need to create a LightFM Dataset, this dataset will allow us later to fit the model with the data in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.data import Dataset\n",
    "dataset1 = Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fit method\n",
    "\n",
    "We need to call the fit method so that LightFM knows who the users are, what items we are dealing with (trainings), and also the user and item features. On the recommender lingo our trainings are just items.\n",
    "\n",
    "We will be passing three parameters to the fit method: the list of users, the list of items, and the user features, passing the list of users and items is pretty straightforward — just use the `User-Id` and `Training-Id` columns from trainingsttaken dataframe.\n",
    "\n",
    "When it comes to pass the user_features, it's better to pass a list in which each element is in the format `feature_name:feature_value`.\n",
    "\n",
    "Then ouruser_features should look something like this:\n",
    "`['name:Susana Johnson', 'Age:42', 'los:IFS', 'ou:development', 'gender:F', 'skills:azure', 'language:dutch']`.\n",
    "\n",
    "This list was generated by considering all possible `feature_name,feature_value pairs` that can be found in the training set. For example, for feature_name equal to Gender, there can be two feature_values namely M and F."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uf = []\n",
    "col = ['ou']*len(usersdf.ou.unique()) + ['skills']*len(usersdf.skills.unique()) + ['language']*len(usersdf.language.unique()) + ['grade']*len(usersdf['grade'].unique()) + ['career interests']*len(usersdf['career interests'].unique())\n",
    "unique_f1 = list(usersdf.ou.unique()) + list(usersdf.skills.unique()) + list(usersdf.language.unique()) + list(usersdf['grade'].unique())+ list(usersdf['career interests'].unique())\n",
    "for x,y in zip(col, unique_f1):\n",
    "    res = str(x)+ \":\" +str(y)\n",
    "    uf.append(res)\n",
    "    print(res\n",
    "    \n",
    "ou:development\n",
    "ou:operations\n",
    "ou:architecture\n",
    "ou:cloud operations\n",
    "ou:pmo\n",
    "skills:azure\n",
    "skills:javascript\n",
    "skills:pm\n",
    "skills:.net\n",
    "skills:python\n",
    "skills:solutions design\n",
    "skills:sql\n",
    "language:dutch\n",
    "language:french\n",
    "language:german\n",
    "language:spanish\n",
    "language:english\n",
    "grade:Junior\n",
    "grade:Associate\n",
    "grade:Senior Manager\n",
    "grade:Manager\n",
    "grade:Senior Associate\n",
    "career interests:solutions design\n",
    "career interests:javascript\n",
    "career interests:pm\n",
    "career interests:python\n",
    "career interests:azure\n",
    "career interests:.net\n",
    "career interests:sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The piece of code above generates the list we need, in the format explained above, with all possible combinations. This is what LightFM expects.\n",
    "\n",
    "Now we need to call the .fit method which accepts a list of User-Ids, Training-IDs, and a list of all user features (list above).\n",
    "\n",
    "After calling the fit method, I converted my trainingdf fields to numeric because that's the expected type, and finally using the dataset1 instance I call build_interactions, on this method what I do is to iterate over all trainingstaken dataframe and pass them as parameters one by one, the User-Id and the Training-Id, optionally you can also pass the weights (ratings), in my case I am ignoring this column, because I assumed, in the beginning, all rows with a 10 value would mean the user took the training, but in this case is irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call fit to supply userid, item id and user/item features\n",
    "dataset1.fit(\n",
    "        usersdf['User-Id'].unique(), # all the users\n",
    "        trainingsdf['Training-Id'].unique(), # all the items\n",
    "        user_features = uf # additional user features\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "trainingstakendf[\"User-Id\"] = pd.to_numeric(trainingstakendf[\"User-Id\"])\n",
    "trainingstakendf[\"Training-Id\"] = pd.to_numeric(trainingstakendf[\"Training-Id\"])\n",
    "\n",
    "# plugging in the interactions and their weights\n",
    "(interactions, weights) = dataset1.build_interactions([(x[0], x[1]) for x in trainingstakendf.values ])\n",
    "\n",
    "interactions.todense()\n",
    "Output:\n",
    "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
    "        [0, 0, 0, ..., 0, 0, 0],\n",
    "        [0, 0, 0, ..., 0, 0, 0],\n",
    "        ...,\n",
    "        [0, 0, 0, ..., 0, 0, 0],\n",
    "        [0, 0, 0, ..., 0, 0, 0],\n",
    "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)\n",
    "        \n",
    "weights.todense()\n",
    "Output:\n",
    "matrix([[0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On line Nr 13 the build interactions method returns a sparse matrix of interactions and weights, if you want a cleaner representation you can use the `.todense `method to view a summary of the dense matrix.\n",
    "\n",
    "### Creating the user features\n",
    "\n",
    "The `build_user_features` method requires parameters like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "(user1 , [feature1, feature2, feature3, ….]),\n",
    "(user2 , [feature1, feature2, feature3, ….]),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Remember that `feature1 , feature2, feature3` , etc should be one of the items present in `user_features` list that we passed to the fit method before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to reiterate, this is how our user_features list currently looks like:\n",
    "`['name:Susana Johnson', 'Age:42', 'los:IFS', 'ou:development', 'gender:F', 'skills:azure', 'language:dutch']`.\n",
    "\n",
    "For our particular example, it should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "     ('1', ['name:Susana Johnson', 'Age:32', 'los:IFS', 'ou:development', 'gender:F', 'skills:azure', 'language:dutch']),\n",
    "     ('2', .....\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method and some of the explanation on this blog post were taken from: https://towardsdatascience.com/how-i-would-explain-building-lightfm-hybrid-recommenders-to-a-5-year-old-b6ee18571309"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific code for my use case is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_colon_value(my_list):\n",
    "    \"\"\"\n",
    "    Takes as input a list and prepends the columns names to respective values in the list.\n",
    "    For example: if my_list = [1,1,0,'del'],\n",
    "    resultant output = ['f1:1', 'f2:1', 'f3:0', 'loc:del']\n",
    "   \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    ll = ['ou:','skills:', 'language:', 'grade:', 'career interests:']\n",
    "    aa = my_list\n",
    "    for x,y in zip(ll,aa):\n",
    "        res = str(x) +\"\"+ str(y)\n",
    "        result.append(res)\n",
    "    return result\n",
    "# Using the helper function to generate user features in proper format for ALL users\n",
    "ad_subset = usersdf[[\"ou\", 'skills','language', 'grade', 'career interests']] \n",
    "ad_list = [list(x) for x in ad_subset.values]\n",
    "feature_list = []\n",
    "for item in ad_list:\n",
    "    feature_list.append(feature_colon_value(item))\n",
    "print(f'Final output: {feature_list}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically from our user dataframe, we remove the columns which we think are not relevant for the training eg (name, age, etc). Then with some python magic, we create an array, where each element of the array is a list of feature name and value for the user on that specific index position.\n",
    "\n",
    "Finally, we need to add the User-Id to each element of the array, which can be done with the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tuple = list(zip(usersdf['User-Id'], feature_list))\n",
    "user_tuple\n",
    "Output:\n",
    "[(8361131,\n",
    "  ['ou:development',\n",
    "   'skills:azure',\n",
    "   'language:dutch',\n",
    "   'grade:Junior',\n",
    "   'career interests:solutions design']),\n",
    " (2162101,\n",
    "  ['ou:development',\n",
    "   'skills:javascript',\n",
    "   'language:french',\n",
    "   'grade:Junior',\n",
    "   'career interests:javascript']),\n",
    " (81727,\n",
    "  ['ou:operations',\n",
    "   'skills:pm',\n",
    "   'language:german',\n",
    "   'grade:Junior',\n",
    "   'career interests:pm']),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost there.\n",
    "\n",
    "Now we have our user feature in the required format, we can call the build_user_features method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = dataset1.build_user_features(user_tuple, normalize= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again this returns a sparse matrix, and if we call the .todense method we can have a more clear representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.todense()\n",
    "matrix([[1., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 1., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 1., ..., 0., 0., 0.],\n",
    "        ...,\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 0., 0., 0.],\n",
    "        [0., 0., 0., ..., 1., 0., 0.]], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `user_features` matrix above, the rows are the users, and columns are the user features. There is a 1 present whenever that user has that particular user feature present in training data.\n",
    "\n",
    "If we call the `.shape` attribute on the `user_features matrix`, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features.shape\n",
    "(9996, 10025)\n",
    "user_id_map, user_feature_map, item_id_map, item_feature_map = dataset1.mapping()\n",
    "user_feature_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method above returns a user id mapping, user feature mapping, item id mapping and item feature mapping, it might be handy when debugging and it will be very practical on the prediction phase.\n",
    "\n",
    "### Lets build the model\n",
    "\n",
    "Everything that we did before is in order to be able to fit the model, as it expects sparse matrix format for the interactions and user features parameters. The fit method expects the interactions, user features, and some other optional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightFM(loss='warp')\n",
    "model.fit(interactions,\n",
    "user_features= user_features,\n",
    "epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we train our model we can evaluate it with AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.evaluation import auc_score\n",
    "train_auc = auc_score(model,\n",
    "interactions,\n",
    "user_features=user_features\n",
    ").mean()\n",
    "print('Hybrid training set AUC: %s' % train_auc)\n",
    "Hybrid training set AUC: 0.9402231"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember it's dummy data, so the model might overfit, don't get too excited with the 94% metric.\n",
    "\n",
    "### Let's predict for known users\n",
    "\n",
    "The predict method takes 2 parameters: the user id mapping, and the list of item ids. Here we will use user_id_map from the previous step to get a reference to the specific user (user_x),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "user_x = user_id_map[9212216] #just a random user\n",
    "n_users, n_items = interactions.shape # no of users * no of items\n",
    "model.predict(user_x, np.arange(n_items)) # means predict for all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will return the score for each item (training) into an array format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array([-0.49955484, -0.4502962 , -0.6466697 , -0.7361969 , -0.30803648,\n",
    "        0.01278364, -0.37532082, -0.2221036 , -0.7242191 , -1.6705698 ,\n",
    "       -0.01221651, -0.23012483, -0.89942145, -1.3498331 , -0.7373183 ,\n",
    "       -0.20021401,  0.21310112, -0.9948864 ,  0.13983092, -0.7846861 ,\n",
    "       -0.5542359 , -0.30498767,  1.0424366 , -0.29013318, -0.23596957,\n",
    "        0.1327716 , -0.49574524, -1.5379183 , -0.7636943 , -0.12699573,\n",
    "        0.14224172, -0.4512871 , -0.49226752,  0.01528413,  0.4442131 ],\n",
    "      dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation: this method returns an array of scores corresponding to the score assigned by the model to _pairs of inputs. Importantly, this means the i-th element of the output array corresponds to the score for the i-th user-item pair in the input arrays.\n",
    "\n",
    "Concretely, you should expect the `lfm.predict([0, 1], [8, 9])` to return an array of np.float32 that may look something like `[0.42 0.31]`, where `0.42` is the score assigned to the user-item pair `(0, 8)` and `0.31` the score assigned to pair `(1, 9)` respectively.\n",
    "\n",
    "If you check LightFM documentation you can also use predict_rank and it will return the items in a sorted order where the first ones are the recommendations for that specific user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The recommendation method\n",
    "\n",
    "And finally as a bonus, I will leave the sample_recommendation_user method, this will take as input the trained model, the interactions matrix, an existing user if, the users dataframe, the tranings dataframe.  And the end result of this method is the trainings taken by the user and the trainings recommended to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_recommendation_user(model, interactions, user_id, usersdf,\n",
    "                               trainingsdf, trainingstakendf,threshold = 0,nrec_items = 25, show = True):\n",
    "    \n",
    "    n_users, n_items = interactions.shape\n",
    "    userInfo = usersdf[usersdf['EID']==user_id]\n",
    "    user_x = np.int64(userInfo.index).item()\n",
    "    scores = pd.Series(model.predict(user_x,np.arange(n_items)))\n",
    "    Taken = \"\"\n",
    "    Recom = \"\"\n",
    "\n",
    "    \n",
    "    resulting = trainingsdf.merge(scores.to_frame().reset_index(), left_index=True, right_index=True)\n",
    "    resulting.drop(columns = ['index'], inplace=True)\n",
    "    resulting.rename(columns={0: \"Score\"}, errors=\"raise\", inplace=True)\n",
    "\n",
    "    resulting.sort_values('Score', ascending=False, inplace=True)\n",
    "    resulting = resulting.head(nrec_items)\n",
    "    resulting.reset_index(drop=True, inplace=True)\n",
    "    userInfo = usersdf[usersdf['EID']==user_id]\n",
    "    userInfo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    users_trainingTaken = trainingstakendf[trainingstakendf['EID']==user_id]\n",
    "    users_trainingTaken.drop_duplicates(inplace=True)\n",
    "\n",
    "    users_trainingTaken = pd.merge(users_trainingTaken, trainingsdf, how=\"inner\", on='TID')\n",
    "    users_trainingTaken.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    if show == True:\n",
    "        for ix, row in users_trainingTaken.iterrows():\n",
    "            Taken = Taken + str({row[\"Training Title\"]})\n",
    "        for ix,row in resulting.iterrows():\n",
    "            Recom = Recom + str({row[\"Training Title\"]})\n",
    "\n",
    "    d = {'ID': user_id, 'Trainings Taken': Taken, 'Trainings Recommended': Recom}\n",
    "    returndf = pd.DataFrame(data = d, index=[user_id])\n",
    "\n",
    "    return returndf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the Model in Azure\n",
    "\n",
    "Finally, we need to be able to use the model in our apps, for that we can register the model in Azure ML and maybe even deploy it as a web service, in order to register the model, we need to save it first as binary (pickle file), and then we can use Azure ML SDK in order to register the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('savefile.pickle', 'wb') as fle:\n",
    "pickle.dump(model, fle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save the pickle file in your current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "ws = Workspace(subscription_id=\"x\",resource_group=\"y\",workspace_name=\"z\")\n",
    "model = Model.register(ws, model_name=\"recommender\", model_path=\"./savefile.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally with the code above we register the model in Azure.\n",
    "\n",
    "![Azure ML Models](https://miro.medium.com/max/556/1*c90R148d2QGHUfPo3yrB0Q.png)\n",
    "\n",
    "Once the model is registered into Azure ML, you can deploy it as a web service or load it from Python code to reuse it in your predictions.\n",
    "\n",
    "## Final Words\n",
    "\n",
    "Today we went through all the steps to understand recommender systems, we referenced a blog post with a clear theoretical introduction to recommender systems, then we generated fake data and registered it in Azure ML as datasets, and finally, we went step by step in preparing the datasets into the specific formats desired by LightFM.\n",
    "\n",
    "Later we made some predictions for a random user ID, and finally we were able to register the LightFM model by exporting the file first as pickle format and then registering this file as a model for later usage in Azure ML.\n",
    "\n",
    "We didn't go into details of evaluation metrics, or tuning parameters or features, but I expect that at the end you have a clear overview of the entire process so that you can apply it to your specific needs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d017ca6a38457c3c231fd987621ea90f1e5c4ffa6c071b7ca2f6f2d9da9d0653"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
